{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home assignment 1\n",
    "\n",
    "You should work on the assignement in groups/teams of 3 participants. \n",
    "\n",
    "Upload your solution as a jupyter notebook to moodle by Wednesday, 20th of November 23:59h. (The deadline is strict)\n",
    "It is sufficient if one student of each team submits the solution.\n",
    "\n",
    "Do not forget to specify the names of all contributing students in the jupyter notebook.\n",
    "\n",
    "You should add comments to your code where necessary and print the relevant results. You should also always test your code on self-chosen examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet path similarity"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Reimplement the path similarity between two words in WordNet using the NLTK package. The path similarity between two words is given by the smallest similarity of any pair of their synsets.\n",
    "Obviously, do not use the pathsim function already implemented. From NLTK you should only use the synset/synsets/hyponym/hypernym functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import deque\n",
    "import math\n",
    "\n",
    "def pathSimilarity(word1, word2):\n",
    "    word_empty = {}\n",
    "    if word1 in word_empty:\n",
    "        word1_synsets = word_empty[word1]\n",
    "    else:\n",
    "        word1_synsets = wn.synsets(word1)\n",
    "        word_empty[word1] = word1_synsets\n",
    "    if word2 in word_empty:\n",
    "        word2_synsets = word_empty[word2]\n",
    "    else:\n",
    "        word2_synsets = wn.synsets(word2)\n",
    "        word_empty[word2] = word2_synsets\n",
    "    if not word1_synsets or not word2_synsets:\n",
    "        return 'enter two words!'\n",
    "    min_distance = 10000\n",
    "    for word1_synset in word1_synsets:\n",
    "        for word2_synset in word2_synsets:\n",
    "            distance = shortest_distance(word1_synset, word2_synset)\n",
    "            if distance is not None and distance < min_distance:\n",
    "                min_distance = distance\n",
    "    if min_distance == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return (1.0/(min_distance+1))\n",
    "    \n",
    "def shortest_distance(word1_synset, word2_synset):   \n",
    "    if word1_synset == word2_synset:\n",
    "       return 0\n",
    "    else:\n",
    "        distance = float('inf')  #inf stands for no link between two nodes\n",
    "        dist_dict1, queue1 = {}, deque([(word1_synset, 0)])  #path and to-do list will be searched later\n",
    "        dist_dict2, queue2 = {}, deque([(word2_synset, 0)])\n",
    "        while queue1:\n",
    "            node, depth = queue1.popleft()  #BFS FIFO\n",
    "            if node in dist_dict1:\n",
    "                continue\n",
    "            dist_dict1[node] = depth  #record the searched node and its depth\n",
    "            for neighbor in node.hypernyms():\n",
    "                queue1.extend([(neighbor, depth + 1)])  #add neighbor node into to-do list\n",
    "        while queue2:\n",
    "            node, depth = queue2.popleft()  #BFS FIFO\n",
    "            if node in dist_dict2:\n",
    "                continue\n",
    "            dist_dict2[node] = depth\n",
    "            for neighbor in node.hypernyms():\n",
    "                queue2.extend([(neighbor, depth + 1)])     \n",
    "        for synset, d1 in dist_dict1.items():\n",
    "            if synset in dist_dict2:    #no commen synset then infinite path distance\n",
    "                d2 = dist_dict2[synset]\n",
    "            else:\n",
    "                d2 = float('inf')\n",
    "            distance = min(distance, d1 + d2) #minimal distance between two nodes in two dictionaries from same synset\n",
    "        if distance == float('inf'):\n",
    "            return None \n",
    "        else:\n",
    "            return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "print(pathSimilarity('cat','dog'))\n",
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "print(dog.path_similarity(cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov chain model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Assume that you are provided a text as a string. Write a function that tokenizes the text and removes removes all the puntuation marks as well as lemmatizes it. The function should return a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def textNormalization(text):\n",
    "    \n",
    "    \n",
    "    tokens = word_tokenize (text)\n",
    "    \n",
    "    # filter twiceï¼Œ first delete all the words that are punctuation, \n",
    "    # then check all the words such as \"'m or \"n't\", WLOG, here we only want\n",
    "    # pure word, and drop the abbreviation\n",
    "\n",
    "    tokens = list(filter(lambda token: token not in string.punctuation, tokens))\n",
    "    tokens = list(filter(lambda token: token.isalpha(), tokens))\n",
    "    \n",
    "    # Lemmatize the word using nltk package\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    lemmatized_output = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    return lemmatized_output\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Assume that you have a normalized text (obtained by applying the textNormalization function in the previous task.)\n",
    "In this task, we will implement Markov chain models of first and second order. The trained models should be stored in the following way:\n",
    "A first-order Markov chain model is given by a dictionary with a 2-tuples keys and floats as values. Each 2-tuple specifies the last word and the next word, the float the respective probability in the model. For example, the entry\n",
    "(\"apple\", \"pie\") : 0.5 \n",
    "describes that the probability of \"pie\" being the next word if \"apple\" was the previous word is 0.5\n",
    "\n",
    "A second-order Markov chain model is given by a dictionary with a 3-tuples keys and floats as values. Each 3-tuple specifies the two last words and the next word, the float the respective probability in the model. For example, the entry\n",
    "(\"the\", \"apple\", \"pie\") : 0.5 \n",
    "describes that the probability of \"pie\" being the next word if \"the\" and \"apple\" were the previous two words is 0.5 according to the model.\n",
    "Note: You can (and should) use defaultdict objects from the collections module with appropriate default values. Nonetheless, do not expect these data formats to scale well for large texts!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Implement the following two functions that determines a maximum-likelihood first-order (second-order resp.) Markov model (in the previously specified format) from a text given as a string. As an optional argument also a Laplace correction can be specified. The correction is given by an integer number that is to be treated as the number of pseudo-observations.\n",
    "\n",
    "Furthermore, implement a function that computes the perplexity of a model (either first- or second order Markov chain models in the specified format) on a given text (string).\n",
    "\n",
    "Follow the following function headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text = \"I'm don't Assume that Assume that Assume that you you have you have a normalized text (obtained by applying the textNormalization function in the previous task.) In this task, we will implement Markov chain models of first and second order. The trained models should be stored in the following way: A first-order Markov chain model is given by a dictionary with a 2-tuples keys and floats as values. Each 2-tuple specifies the last word and the next word, the float the respective probability in the model. For example, the entry ('apple', 'pie') : 0.5 describes that the probability of 'pie' being the next word if 'apple' was the previous word is 0.5 A second-order Markov chain model is given by a dictionary with a 3-tuples keys and floats as values. Each 3-tuple specifies the two last words and the next word, the float the respective probability in the model. For example, the entry : 0.5 describes that the probability of 'pie' being the next word if 'the' and 'apple' were the previous two words is 0.5 according to the model. Note: You can (and should) use defaultdict objects from the collections module with appropriate default values. Nonetheless, do not expect these data formats to scale well for large texts!\"\n",
    "testing_text = \"Assume that you have a normalized text, you can do whatever things with it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "import collections as cl\n",
    "\n",
    "def first_order_markov(training_text, laplace_correction = 0):\n",
    "        \n",
    "    # normalized text first\n",
    "    tokens = textNormalization(training_text)\n",
    "    v = len(set(tokens))\n",
    "  \n",
    "    # find the frequency distance of bigrams (items in dictionary is unique)\n",
    "    tuple_word_dictionary = nltk.FreqDist(nltk.bigrams(tokens))\n",
    "    single_word_dictionary = nltk.FreqDist(tokens)\n",
    "    \n",
    "    #calculated the probability for each bigrams, WLOG, we default all the words that not showed in dictionary\n",
    "    # to be 0.001. So that when we calculate the preplexity we have a range to determine the accuracy of the model.\n",
    "    \n",
    "    dictionary = cl.defaultdict(lambda:0.001)\n",
    "    for tup in tuple_word_dictionary:\n",
    "        dictionary[tup] = (tuple_word_dictionary[tup]+laplace_correction)/(single_word_dictionary[tup[0]]+laplace_correction*v)\n",
    "    return dictionary \n",
    "\n",
    "\n",
    "def second_order_markov(training_text, laplace_correction = 0):\n",
    "    # normalized text first\n",
    "    tokens = textNormalization(training_text)\n",
    "    \n",
    "    # find the frequency distance of trigrams (items in dictionary is unique)\n",
    "    trigrams=nltk.FreqDist(ngrams(tokens,3))\n",
    "    bigrams=nltk.FreqDist(nltk.bigrams(tokens))\n",
    "    v = len(set(bigrams))\n",
    "    \n",
    "    #calculated the probability for each bigrams, WLOG, we default all the words that not showed in dictionary\n",
    "    # to be 0.001. So that when we calculate the preplexity we have a range to determine the accuracy of the model.\n",
    "    dictionary = cl.defaultdict(lambda:0.001)\n",
    "    for tri in trigrams:\n",
    "        bi = tri[0:2]\n",
    "        dictionary[tri] = ((trigrams[tri]+laplace_correction)/(bigrams[bi]+laplace_correction*v))\n",
    "    return dictionary\n",
    "\n",
    "\n",
    "# @ Input: evaluation_text, model type\n",
    "# @ Output: a float that indicate how much the model fit the evaluation_text. The smaller the float, the model is more fit to\n",
    "# the evaluation_text. Here we use default value 0.001, so the whole range should be 1~ pow(1000**N,N)\n",
    "\n",
    "def perplexity (evaluation_text, model):\n",
    "    \n",
    "    tokens = textNormalization(evaluation_text)\n",
    "\n",
    "    # training _text is outside which use for calcuating the model\n",
    "    # dictionary is the evaluation_text bigrams/Trigrams\n",
    "    if (model == \"first\"):\n",
    "        model= first_order_markov(training_text,1)\n",
    "        dictionary = nltk.bigrams(tokens)\n",
    "    elif (model == \"second\"):\n",
    "        model = second_order_markov(training_text,1)\n",
    "        dictionary = ngrams(tokens,3)\n",
    "    else: \n",
    "        return \"ERROR: TYPE DO NOT EXIST\"\n",
    "    \n",
    "    per = 1\n",
    "    N = 0\n",
    "    for i in dictionary:\n",
    "        print(i)\n",
    "        print(model[i])\n",
    "        per *= (1/model[i]) \n",
    "        N +=1\n",
    "    \n",
    "    return pow(per,1/N)\n",
    "        \n",
    "perplexity(testing_text,\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a function that implements the Viterbi algorithm as introduced in the lecture. The function takes as input a sentence, state transition probability and word emission probability and returns the sequence of POS tags. State transition and word emission probabilities are also provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "\n",
    "def Viterbi(State_trans_prob, Word_emission_prob,sentence):\n",
    "    states = set()\n",
    "    for x  in list(State_trans_prob.keys()):\n",
    "        states.add(x[0])\n",
    "        states.add(x[1])\n",
    "    states.remove('<s>')\n",
    "    obs = sentence.split(' ')\n",
    "    \n",
    "    # define the problity Matrix of viterbi[N,T],and backpointer\n",
    "    viterbi = {}\n",
    "    backpointer = {}\n",
    "    for state in states:\n",
    "        for ob in obs:\n",
    "            viterbi[state,ob] = 0\n",
    "            backpointer[state,ob] = 0\n",
    "\n",
    "    # init\n",
    "    for state in states:\n",
    "        if (obs[0],state) in Word_emission_prob.keys():\n",
    "            viterbi[state,obs[0]]=State_trans_prob['<s>',state]*Word_emission_prob[obs[0],state]\n",
    "        \n",
    "    # iteration\n",
    "    for i,ob in enumerate(obs):\n",
    "        if i > 0:\n",
    "            for state in states:  \n",
    "                if(ob,state) in Word_emission_prob.keys():\n",
    "                    candidate = {}\n",
    "                    for pre_state in states:\n",
    "                        if (pre_state,state) in State_trans_prob.keys():\n",
    "                            candidate[pre_state] = viterbi[pre_state,obs[i-1]] * State_trans_prob[pre_state,state]* Word_emission_prob[ob,state]\n",
    "                    viterbi[state,ob] = max(candidate.values())\n",
    "                    \n",
    "#                     viterbi[state,ob] = max(viterbi[pre_state,obs[i-1]] * State_trans_prob[pre_state,state]* Word_emission_prob[ob,state] \\\n",
    "#                                                 for pre_state in states if (pre_state,state) in State_trans_prob.keys())                \n",
    "   \n",
    "                    for key,val in candidate.items():\n",
    "                        if val == max(candidate.values()):\n",
    "                            backpointer[state,ob] = key                           \n",
    "                 \n",
    "    #terminate\n",
    "    for key,val in viterbi.items():\n",
    "        if val == max(viterbi[state,obs[len(obs)-1]]for state in states):\n",
    "            tag = key[0]\n",
    "    obs.reverse()\n",
    "    print(obs)\n",
    "    \n",
    "    #backword compute the state for the word in the sentence.\n",
    "    out={}\n",
    "    for ob in obs:\n",
    "        out[ob] = tag\n",
    "        tag = backpointer[tag,ob]\n",
    "    final = list(out.values())\n",
    "    final.reverse()\n",
    "    \n",
    "    return final\n",
    "\n",
    "# Viterbi(State_trans_prob, Word_emission_prob, Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "State_trans_prob = {('<s>','NNP'):0.2767,('<s>','MD'):0.006,('<s>','VB'):0.0031,('<s>','JJ'):0.0453,('<s>','NN'):0.0449,\n",
    "                   ('<s>','RB'):0.0510,('<s>','DT'):0.2026,\n",
    "                   ('NNP','NNP'):0.3777,('NNP','MD'):0.0110,('NNP','VB'):0.0009,('NNP','JJ'):0.0084,('NNP','NN'):0.0584,\n",
    "                   ('NNP','RB'):0.0090,('NNP','DT'):0.0025,\n",
    "                   ('MD','NNP'):0.0008,('MD','MD'):0.0002,('MD','VB'):0.7968,('MD','JJ'):0.0005,('MD','NN'):0.0008,\n",
    "                   ('MD','RB'):0.1698,('MD','DT'):0.0041,\n",
    "                   ('VB','NNP'):0.0322,('VB','MD'):0.0005,('VB','VB'):0.0050,('VB','JJ'):0.0837,('VB','NN'):0.0615,\n",
    "                   ('VB','RB'):0.0514,('VB','DT'):0.2231,\n",
    "                   ('JJ','NNP'):0.0366,('JJ','MD'):0.0004,('JJ','VB'):0.0001,('JJ','JJ'):0.0733,('JJ','NN'):0.4509,\n",
    "                   ('JJ','RB'):0.0036,('JJ','DT'):0.0036,\n",
    "                   ('NN','NNP'):0.0096,('NN','MD'):0.0176,('NN','VB'):0.0014,('NN','JJ'):0.0086,('NN','NN'):0.1216,\n",
    "                   ('NN','RB'):0.0177,('NN','DT'):0.0068,\n",
    "                   ('RB','NNP'):0.0068,('RB','MD'):0.0102,('RB','VB'):0.1011,('RB','JJ'):0.1012,('RB','NN'):0.0120,\n",
    "                   ('RB','RB'):0.0728,('RB','DT'):0.0479,\n",
    "                   ('DT','NNP'):0.1147,('DT','MD'):0.0021,('DT','VB'):0.0002,('DT','JJ'):0.2157,('DT','NN'):0.4744,\n",
    "                   ('DT','RB'):0.0102,('DT','DT'):0.0017}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word_emission_prob = {('Janet','NNP'):0.000032, ('will','MD'):0.308431,('will','VB'):0.000028,('will','NN'):0.0002,\n",
    "                     ('back','VB'):0.000672,('back','JJ'):0.00034,('back','NN'):0.000223,('back','RB'):0.010446,\n",
    "                     ('the','NNP'):0.000048,('the','DT'):0.506099,('bill','VB'):0.000028,('bill','NN'):0.002337}\n",
    "# assume the missing entries are 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bill', 'the', 'back', 'will', 'Janet']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['NNP', 'MD', 'RB', 'DT', 'NN'],\n",
       " {('NN', 'Janet'): 0,\n",
       "  ('NN', 'will'): 'NNP',\n",
       "  ('NN', 'back'): 'MD',\n",
       "  ('NN', 'the'): 0,\n",
       "  ('NN', 'bill'): 'DT',\n",
       "  ('NNP', 'Janet'): 0,\n",
       "  ('NNP', 'will'): 0,\n",
       "  ('NNP', 'back'): 0,\n",
       "  ('NNP', 'the'): 'VB',\n",
       "  ('NNP', 'bill'): 0,\n",
       "  ('JJ', 'Janet'): 0,\n",
       "  ('JJ', 'will'): 0,\n",
       "  ('JJ', 'back'): 'MD',\n",
       "  ('JJ', 'the'): 0,\n",
       "  ('JJ', 'bill'): 0,\n",
       "  ('DT', 'Janet'): 0,\n",
       "  ('DT', 'will'): 0,\n",
       "  ('DT', 'back'): 0,\n",
       "  ('DT', 'the'): 'RB',\n",
       "  ('DT', 'bill'): 0,\n",
       "  ('MD', 'Janet'): 0,\n",
       "  ('MD', 'will'): 'NNP',\n",
       "  ('MD', 'back'): 0,\n",
       "  ('MD', 'the'): 0,\n",
       "  ('MD', 'bill'): 0,\n",
       "  ('RB', 'Janet'): 0,\n",
       "  ('RB', 'will'): 0,\n",
       "  ('RB', 'back'): 'MD',\n",
       "  ('RB', 'the'): 0,\n",
       "  ('RB', 'bill'): 0,\n",
       "  ('VB', 'Janet'): 0,\n",
       "  ('VB', 'will'): 'NNP',\n",
       "  ('VB', 'back'): 'MD',\n",
       "  ('VB', 'the'): 0,\n",
       "  ('VB', 'bill'): 'DT'},\n",
       " {('NN', 'Janet'): 0,\n",
       "  ('NN', 'will'): 1.03419392e-10,\n",
       "  ('NN', 'back'): 5.35925836641536e-15,\n",
       "  ('NN', 'the'): 0,\n",
       "  ('NN', 'bill'): 1.4320953590187012e-15,\n",
       "  ('NNP', 'Janet'): 8.8544e-06,\n",
       "  ('NNP', 'will'): 0,\n",
       "  ('NNP', 'back'): 0,\n",
       "  ('NNP', 'the'): 2.486139834207686e-17,\n",
       "  ('NNP', 'bill'): 0,\n",
       "  ('JJ', 'Janet'): 0,\n",
       "  ('JJ', 'will'): 0,\n",
       "  ('JJ', 'back'): 5.106916604768e-15,\n",
       "  ('JJ', 'the'): 0,\n",
       "  ('JJ', 'bill'): 0,\n",
       "  ('DT', 'Janet'): 0,\n",
       "  ('DT', 'will'): 0,\n",
       "  ('DT', 'back'): 0,\n",
       "  ('DT', 'the'): 1.2917204778711097e-12,\n",
       "  ('DT', 'bill'): 0,\n",
       "  ('MD', 'Janet'): 0,\n",
       "  ('MD', 'will'): 3.00406859104e-08,\n",
       "  ('MD', 'back'): 0,\n",
       "  ('MD', 'the'): 0,\n",
       "  ('MD', 'bill'): 0,\n",
       "  ('RB', 'Janet'): 0,\n",
       "  ('RB', 'will'): 0,\n",
       "  ('RB', 'back'): 5.3284089852402517e-11,\n",
       "  ('RB', 'the'): 0,\n",
       "  ('RB', 'bill'): 0,\n",
       "  ('VB', 'Janet'): 0,\n",
       "  ('VB', 'will'): 2.2313087999999997e-13,\n",
       "  ('VB', 'back'): 1.6085273254449314e-11,\n",
       "  ('VB', 'the'): 0,\n",
       "  ('VB', 'bill'): 7.233634676078214e-21})"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentence = 'Janet will back the bill'\n",
    "Viterbi(State_trans_prob, Word_emission_prob, Sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging with nltk"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write a function that utilizes the POS tagger in nltk to obtain the POS tags of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\unicorn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n",
    "def posTag(sentence):\n",
    "    text = word_tokenize(sentence)\n",
    "    return nltk.pos_tag(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Janet', 'NNP'),\n",
       " ('will', 'MD'),\n",
       " ('back', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('bill', 'NN')]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentence = 'Janet will back the bill'\n",
    "posTag(Sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
