{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home assignment 2\n",
    "\n",
    "You should work on the assignement in groups/teams of 3 participants. Submissions of single students will not be accepted! Please use the Forum in case of doubt in order to find team mates!\n",
    "\n",
    "Upload your solution as a jupyter notebook to moodle by Tuesday, 7th of January 23:55h. (The deadline is strict)\n",
    "It is sufficient if one student of each team submits the solution.\n",
    "\n",
    "\n",
    "You should add comments to your code where necessary and print the relevant results. You should also always test your code on self-chosen examples.\n",
    "\n",
    "Do not forget to specify the (First_name, Last_name, student_id (matrikelnummer)) of all contributing students in the jupyter notebook here:\n",
    "\n",
    "Student 1: `Yaping, Chen, 379645`\n",
    "\n",
    "Student 2: `Qingqing, Yang, 393415`\n",
    "\n",
    "Student 3: `First_name, Last_name, student_id`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing GloVe\n",
    "In this task you will implement the glove algorithm using PyTorch. (One advantage is that you need not calculate gradient by hand, but you can take advantage of the autograd module). The task will require implementation of certain functions, which we look into step-by-step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for word to index mapping. Since the model won't be able to take strings as input we will convert them into indices. The function will generate a mapping  w2i which uses words as keys and corresponding indices as values e.g., `w2i['walk'] = 42`. As Preprocessing, remove all punctuations and convert all words to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def word2indexMapping(textfile):\n",
    "    w2i = {}\n",
    "    text = [] # sequence of words as they appear in the text after removing punctuations\n",
    "    \n",
    "    #Preprocessing\n",
    "    textfile = textfile.replace('\\n', ' ').strip()\n",
    "    translator = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    text = textfile.translate(translator)\n",
    "    text = text.lower()\n",
    "    \n",
    "    #Mapping\n",
    "    test_list = word_tokenize(text)\n",
    "    w2i = {k: test_list.index(k) for k in test_list}\n",
    "\n",
    "    return w2i, text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for calculating a two dimensional matrix $X_{ij}$ which is the number of times word $j$ occurred in the context of word $i$. The size of the context window $k$ (as a number of words, $k=2$ describes that the context contains the two words before and the two words after the central word) is also an argument of the function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrenceFreq(text, w2i, k): # text is a sequence of words ordered as they appear in the text\n",
    "    # note that there is no notion of sentence here...\n",
    "    \n",
    "    X_ij =  np.zeros((len(w2i), len(w2i)))\n",
    "    \n",
    "    # write your code snippet here...\n",
    "    text = text.split(\" \")\n",
    "    tokens = list(w2i.keys())\n",
    "    token_ids = list(w2i.values())\n",
    "    context_ids = [] #context for each word\n",
    "    for center_id in token_ids:\n",
    "        #left part context for given center\n",
    "        x = max(0, center_id - k)\n",
    "        #right part context for given center_id\n",
    "        y = min(len(text) - 1, center_id + k)\n",
    "        context_ids.append(list(range(x, y + 1)))\n",
    "        contexts_len = len(context_ids)\n",
    "    #print(context_ids)\n",
    "    count, y = 0, 0 #y represent for index for each center word\n",
    "    for center_i in tokens:\n",
    "        for i in range(0,contexts_len): #two demension list interation by i, j\n",
    "            count = 0\n",
    "            for j in range(0,len(context_ids[i])):\n",
    "                if center_i!=text[context_ids[i][j]]:\n",
    "                    j=j+1\n",
    "                else: count=count+1\n",
    "            X_ij[i][y]=count\n",
    "            i+=1\n",
    "        y+=1\n",
    "    return X_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a GloVe model class with parameters $w$, $\\hat w$, $b$ and $\\hat b$. For a particular pair of words $i$, $j$, your forward function should implement $w_{i}^{T}\\hat w_{j} + b_{i} + \\hat b_{j}$. Assume a dimension of embedding to be $d$ which you will specify when creating an instance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove(nn.Module):\n",
    "    # write your model class here....\n",
    "    def __init__(self, text, dimension):\n",
    "        super().__init__()\n",
    "        self.dimension = dimension\n",
    "        self.vocab_size = len(text) #text after preprocess, so only unique words contained\n",
    "        #Word vector matrix (2V) * d\n",
    "        #All elements are initialized randomly in the range (-0.5,0.5]\n",
    "        self.w_1 = nn.Parameter((torch.randn(self.vocab_size * 2, dimension) - 0.5) / float(dimension + 1), requires_grad=True)\n",
    "        self.w_2 = nn.Parameter((torch.randn(self.vocab_size * 2, dimension) - 0.5) / float(dimension + 1), requires_grad=True)\n",
    "        #bias array size (2V)\n",
    "        #initialized randomly in the range (-0.5, 0.5]\n",
    "        self.b_1 = nn.Parameter((torch.randn(self.vocab_size * 2) - 0.5) / float(dimension + 1), requires_grad=True)\n",
    "        self.b_2 = nn.Parameter((torch.randn(self.vocab_size * 2) - 0.5) / float(dimension + 1), requires_grad=True)\n",
    "\n",
    "        \n",
    "    def forward(self, i, j):\n",
    "        cost = self.w_1[i].dot(self.w_2[j]) + self.b_1[i] + self.b_2[j]#first transpose, then multiply, add\n",
    "        return cost\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that implements the weighting function $f(X_{ij})$\n",
    "\n",
    "$f(x) = (\\frac{x}{100})^{\\frac{3}{4}}$ if x<100 \n",
    "\n",
    "$f(x) = 1 $ otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightFunction(X_ij, i, j):\n",
    "    f = 0\n",
    "    # write your code snippet here\n",
    "    x_max = 100\n",
    "    alpha = 0.75\n",
    "    f = (X_ij[i][j] / x_max) ** alpha if X_ij[i][j] < x_max else 1\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a train function to train the model using stochastic gradient descent. Your each training instance would be a word-context pair ($i$, $j$) and the corresponding loss function would be \n",
    "\n",
    "$f(X_{ij})(w_{i}^{T}\\hat w_{j} + b_{i} + \\hat b_{j} - log(1 + X_{ij}))^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import math\n",
    "\n",
    "def loss_func(X_ij, i, j):\n",
    "    return weightFunction(X_ij, i, j)*((model.forward(i, j)-math.log(1+X_ij[i][j]))**2)\n",
    "\n",
    "def train(model, X_ij, learning_rate=0.001, epochs=5):\n",
    "    opt = optim.Adam(model.parameters(), lr=0.001) # use Adam as your optimizer\n",
    "    for _ in range(epochs):\n",
    "        # train across each word-conext pair\n",
    "        for i in range(len(X_ij)): \n",
    "            for j in range(len(X_ij[0])):\n",
    "                loss = loss_func(X_ij, i, j)# calculate loss\n",
    "                loss.backward() # backpropagate for every training example\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "                #print(loss_func(X_ij, i, j))\n",
    "    return model.w_1, model.w_2, model.b_1, model.b_2        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to generate embeddings given a word. The embedding of a word with index $i$ would $w_i + \\hat w_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbedding(model, word):\n",
    "    # write your code snippet here...\n",
    "    index = w2i[word]\n",
    "    embed = model.w_1[index] + model.w_2[index]\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the individual components together to train a Glove model from the given text file  'shakespeare-caesar.txt' with dimension 200 and a context window of 5.\n",
    "Manually inspect nearest neigbors for some self-picked words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0657, -0.0572, -0.1281,  ..., -0.0904, -0.1343, -0.0241],\n",
      "        [-0.1670,  0.0372, -0.1388,  ..., -0.1712, -0.1954, -0.2449],\n",
      "        [-0.1265, -0.0847, -0.0052,  ...,  0.0859, -0.2149, -0.0122],\n",
      "        ...,\n",
      "        [-0.0480, -0.1029,  0.0109,  ..., -0.0419, -0.0789, -0.2526],\n",
      "        [-0.1959, -0.1430, -0.0826,  ..., -0.0165, -0.0856,  0.1222],\n",
      "        [-0.0970,  0.2424, -0.0202,  ...,  0.1278, -0.0244, -0.1326]],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3f55dfca7f78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_ij\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-733b52757b16>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, X_ij, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# backpropagate for every training example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0;31m#print(loss_func(X_ij, i, j))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('/Users/yangqingqing/Downloads/data_home_assignment_2/shakespeare-caesar.txt', encoding='latin-1') as f:\n",
    "    Text = f.read()\n",
    "w2i, text = word2indexMapping(Text)\n",
    "X_ij = co_occurrenceFreq(text, w2i, k = 5)\n",
    "model = Glove(text, dimension=10)\n",
    "print(model.w_1, model.w_2, model.b_1, model.b_2)\n",
    "train(model, X_ij, learning_rate=0.001, epochs=5)\n",
    "print(model.w_1, model.w_2, model.b_1, model.b_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings with gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim is a package which allows you to train word embeddings as well. The task is to take a text file (like 'shakespeare-caesar.txt') and generate embeddings of the vocabulary. You can consult the documentation here - \n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'of': <gensim.models.keyedvectors.Vocab at 0x1e83fdac508>,\n",
       " 'caesar': <gensim.models.keyedvectors.Vocab at 0x1e860a1f808>,\n",
       " 'by': <gensim.models.keyedvectors.Vocab at 0x1e860a1f848>,\n",
       " 'actus': <gensim.models.keyedvectors.Vocab at 0x1e860a1f888>,\n",
       " 'enter': <gensim.models.keyedvectors.Vocab at 0x1e860a1fdc8>,\n",
       " 'and': <gensim.models.keyedvectors.Vocab at 0x1e860a1fe48>,\n",
       " 'ouer': <gensim.models.keyedvectors.Vocab at 0x1e860a1fe88>,\n",
       " 'the': <gensim.models.keyedvectors.Vocab at 0x1e860a1fec8>,\n",
       " 'home': <gensim.models.keyedvectors.Vocab at 0x1e860a1f8c8>,\n",
       " 'you': <gensim.models.keyedvectors.Vocab at 0x1e860a1fe08>,\n",
       " 'get': <gensim.models.keyedvectors.Vocab at 0x1e860a1ff08>,\n",
       " 'is': <gensim.models.keyedvectors.Vocab at 0x1e860a1ff48>,\n",
       " 'this': <gensim.models.keyedvectors.Vocab at 0x1e860a1ff88>,\n",
       " 'a': <gensim.models.keyedvectors.Vocab at 0x1e860a1ffc8>,\n",
       " 'what,': <gensim.models.keyedvectors.Vocab at 0x1e860a22048>,\n",
       " 'know': <gensim.models.keyedvectors.Vocab at 0x1e860a22088>,\n",
       " 'not': <gensim.models.keyedvectors.Vocab at 0x1e860a220c8>,\n",
       " 'walke': <gensim.models.keyedvectors.Vocab at 0x1e860a22108>,\n",
       " 'vpon': <gensim.models.keyedvectors.Vocab at 0x1e860a22148>,\n",
       " 'day,': <gensim.models.keyedvectors.Vocab at 0x1e860a22188>,\n",
       " 'without': <gensim.models.keyedvectors.Vocab at 0x1e860a221c8>,\n",
       " 'your': <gensim.models.keyedvectors.Vocab at 0x1e860a22208>,\n",
       " 'speake,': <gensim.models.keyedvectors.Vocab at 0x1e860a22248>,\n",
       " 'what': <gensim.models.keyedvectors.Vocab at 0x1e860a22288>,\n",
       " 'trade': <gensim.models.keyedvectors.Vocab at 0x1e860a222c8>,\n",
       " 'art': <gensim.models.keyedvectors.Vocab at 0x1e860a22308>,\n",
       " 'why': <gensim.models.keyedvectors.Vocab at 0x1e860a22348>,\n",
       " 'sir,': <gensim.models.keyedvectors.Vocab at 0x1e860a22388>,\n",
       " 'mur.': <gensim.models.keyedvectors.Vocab at 0x1e860a223c8>,\n",
       " 'where': <gensim.models.keyedvectors.Vocab at 0x1e860a22408>,\n",
       " 'thy': <gensim.models.keyedvectors.Vocab at 0x1e860a22448>,\n",
       " 'thou': <gensim.models.keyedvectors.Vocab at 0x1e860a22488>,\n",
       " 'with': <gensim.models.keyedvectors.Vocab at 0x1e860a224c8>,\n",
       " 'best': <gensim.models.keyedvectors.Vocab at 0x1e860a22508>,\n",
       " 'are': <gensim.models.keyedvectors.Vocab at 0x1e860a22548>,\n",
       " 'you?': <gensim.models.keyedvectors.Vocab at 0x1e860a22588>,\n",
       " 'in': <gensim.models.keyedvectors.Vocab at 0x1e860a225c8>,\n",
       " 'respect': <gensim.models.keyedvectors.Vocab at 0x1e860a22608>,\n",
       " 'i': <gensim.models.keyedvectors.Vocab at 0x1e860a22648>,\n",
       " 'am': <gensim.models.keyedvectors.Vocab at 0x1e860a22688>,\n",
       " 'but': <gensim.models.keyedvectors.Vocab at 0x1e860a226c8>,\n",
       " 'as': <gensim.models.keyedvectors.Vocab at 0x1e860a22708>,\n",
       " 'would': <gensim.models.keyedvectors.Vocab at 0x1e860a22748>,\n",
       " 'say,': <gensim.models.keyedvectors.Vocab at 0x1e860a22788>,\n",
       " 'answer': <gensim.models.keyedvectors.Vocab at 0x1e860a227c8>,\n",
       " 'me': <gensim.models.keyedvectors.Vocab at 0x1e860a22808>,\n",
       " 'directly': <gensim.models.keyedvectors.Vocab at 0x1e860a22848>,\n",
       " 'that': <gensim.models.keyedvectors.Vocab at 0x1e860a22888>,\n",
       " 'may': <gensim.models.keyedvectors.Vocab at 0x1e860a228c8>,\n",
       " 'vse,': <gensim.models.keyedvectors.Vocab at 0x1e860a22908>,\n",
       " 'which': <gensim.models.keyedvectors.Vocab at 0x1e860a22948>,\n",
       " 'bad': <gensim.models.keyedvectors.Vocab at 0x1e860a22988>,\n",
       " 'fla.': <gensim.models.keyedvectors.Vocab at 0x1e860a229c8>,\n",
       " 'be': <gensim.models.keyedvectors.Vocab at 0x1e860a22a08>,\n",
       " 'out': <gensim.models.keyedvectors.Vocab at 0x1e860a22a48>,\n",
       " 'me:': <gensim.models.keyedvectors.Vocab at 0x1e860a22a88>,\n",
       " 'yet': <gensim.models.keyedvectors.Vocab at 0x1e860a22ac8>,\n",
       " 'if': <gensim.models.keyedvectors.Vocab at 0x1e860a22b08>,\n",
       " 'can': <gensim.models.keyedvectors.Vocab at 0x1e860a22b48>,\n",
       " 'all': <gensim.models.keyedvectors.Vocab at 0x1e860a22b88>,\n",
       " 'liue': <gensim.models.keyedvectors.Vocab at 0x1e860a22bc8>,\n",
       " 'by,': <gensim.models.keyedvectors.Vocab at 0x1e860a22c08>,\n",
       " 'no': <gensim.models.keyedvectors.Vocab at 0x1e860a22c48>,\n",
       " 'nor': <gensim.models.keyedvectors.Vocab at 0x1e860a22c88>,\n",
       " 'to': <gensim.models.keyedvectors.Vocab at 0x1e860a22cc8>,\n",
       " 'old': <gensim.models.keyedvectors.Vocab at 0x1e860a22d08>,\n",
       " 'when': <gensim.models.keyedvectors.Vocab at 0x1e860a22d48>,\n",
       " 'they': <gensim.models.keyedvectors.Vocab at 0x1e860a22d88>,\n",
       " 'great': <gensim.models.keyedvectors.Vocab at 0x1e860a22dc8>,\n",
       " 'them.': <gensim.models.keyedvectors.Vocab at 0x1e860a22e08>,\n",
       " 'men': <gensim.models.keyedvectors.Vocab at 0x1e860a22e48>,\n",
       " 'euer': <gensim.models.keyedvectors.Vocab at 0x1e860a22e88>,\n",
       " 'haue': <gensim.models.keyedvectors.Vocab at 0x1e860a22ec8>,\n",
       " 'my': <gensim.models.keyedvectors.Vocab at 0x1e860a22f08>,\n",
       " 'wherefore': <gensim.models.keyedvectors.Vocab at 0x1e860a22f48>,\n",
       " 'leade': <gensim.models.keyedvectors.Vocab at 0x1e860a22f88>,\n",
       " 'these': <gensim.models.keyedvectors.Vocab at 0x1e860a22fc8>,\n",
       " 'about': <gensim.models.keyedvectors.Vocab at 0x1e860a25048>,\n",
       " 'their': <gensim.models.keyedvectors.Vocab at 0x1e860a25088>,\n",
       " 'selfe': <gensim.models.keyedvectors.Vocab at 0x1e860a250c8>,\n",
       " 'into': <gensim.models.keyedvectors.Vocab at 0x1e860a25108>,\n",
       " 'more': <gensim.models.keyedvectors.Vocab at 0x1e860a25148>,\n",
       " 'we': <gensim.models.keyedvectors.Vocab at 0x1e860a25188>,\n",
       " 'make': <gensim.models.keyedvectors.Vocab at 0x1e860a251c8>,\n",
       " 'see': <gensim.models.keyedvectors.Vocab at 0x1e860a25208>,\n",
       " 'caesar,': <gensim.models.keyedvectors.Vocab at 0x1e860a25248>,\n",
       " 'his': <gensim.models.keyedvectors.Vocab at 0x1e860a25288>,\n",
       " 'he': <gensim.models.keyedvectors.Vocab at 0x1e860a252c8>,\n",
       " 'follow': <gensim.models.keyedvectors.Vocab at 0x1e860a25308>,\n",
       " 'him': <gensim.models.keyedvectors.Vocab at 0x1e860a25348>,\n",
       " 'rome,': <gensim.models.keyedvectors.Vocab at 0x1e860a25388>,\n",
       " 'then': <gensim.models.keyedvectors.Vocab at 0x1e860a253c8>,\n",
       " 'o': <gensim.models.keyedvectors.Vocab at 0x1e860a25408>,\n",
       " 'many': <gensim.models.keyedvectors.Vocab at 0x1e860a25448>,\n",
       " 'time': <gensim.models.keyedvectors.Vocab at 0x1e860a25488>,\n",
       " 'vp': <gensim.models.keyedvectors.Vocab at 0x1e860a254c8>,\n",
       " 'there': <gensim.models.keyedvectors.Vocab at 0x1e860a25508>,\n",
       " 'passe': <gensim.models.keyedvectors.Vocab at 0x1e860a25548>,\n",
       " 'saw': <gensim.models.keyedvectors.Vocab at 0x1e860a25588>,\n",
       " 'made': <gensim.models.keyedvectors.Vocab at 0x1e860a255c8>,\n",
       " 'an': <gensim.models.keyedvectors.Vocab at 0x1e860a25608>,\n",
       " 'her': <gensim.models.keyedvectors.Vocab at 0x1e860a25648>,\n",
       " 'heare': <gensim.models.keyedvectors.Vocab at 0x1e860a25688>,\n",
       " 'do': <gensim.models.keyedvectors.Vocab at 0x1e860a256c8>,\n",
       " 'now': <gensim.models.keyedvectors.Vocab at 0x1e860a25708>,\n",
       " 'put': <gensim.models.keyedvectors.Vocab at 0x1e860a25748>,\n",
       " 'on': <gensim.models.keyedvectors.Vocab at 0x1e860a25788>,\n",
       " 'comes': <gensim.models.keyedvectors.Vocab at 0x1e860a257c8>,\n",
       " 'pompeyes': <gensim.models.keyedvectors.Vocab at 0x1e860a25808>,\n",
       " 'fall': <gensim.models.keyedvectors.Vocab at 0x1e860a25848>,\n",
       " 'pray': <gensim.models.keyedvectors.Vocab at 0x1e860a25888>,\n",
       " 'gods': <gensim.models.keyedvectors.Vocab at 0x1e860a258c8>,\n",
       " 'must': <gensim.models.keyedvectors.Vocab at 0x1e860a25908>,\n",
       " 'good': <gensim.models.keyedvectors.Vocab at 0x1e860a25948>,\n",
       " 'countrymen,': <gensim.models.keyedvectors.Vocab at 0x1e860a25988>,\n",
       " 'for': <gensim.models.keyedvectors.Vocab at 0x1e860a259c8>,\n",
       " 'poore': <gensim.models.keyedvectors.Vocab at 0x1e860a25a08>,\n",
       " 'them': <gensim.models.keyedvectors.Vocab at 0x1e860a25a48>,\n",
       " 'till': <gensim.models.keyedvectors.Vocab at 0x1e860a25a88>,\n",
       " 'most': <gensim.models.keyedvectors.Vocab at 0x1e860a25ac8>,\n",
       " 'all.': <gensim.models.keyedvectors.Vocab at 0x1e860a25b08>,\n",
       " 'exeunt.': <gensim.models.keyedvectors.Vocab at 0x1e860a25b48>,\n",
       " 'go': <gensim.models.keyedvectors.Vocab at 0x1e860a25b88>,\n",
       " 'downe': <gensim.models.keyedvectors.Vocab at 0x1e860a25bc8>,\n",
       " 'way': <gensim.models.keyedvectors.Vocab at 0x1e860a25c08>,\n",
       " 'will': <gensim.models.keyedvectors.Vocab at 0x1e860a25c48>,\n",
       " 'finde': <gensim.models.keyedvectors.Vocab at 0x1e860a25c88>,\n",
       " 'so?': <gensim.models.keyedvectors.Vocab at 0x1e860a25cc8>,\n",
       " 'it': <gensim.models.keyedvectors.Vocab at 0x1e860a25d08>,\n",
       " 'matter,': <gensim.models.keyedvectors.Vocab at 0x1e860a25d48>,\n",
       " 'let': <gensim.models.keyedvectors.Vocab at 0x1e860a25d88>,\n",
       " 'caesars': <gensim.models.keyedvectors.Vocab at 0x1e860a25dc8>,\n",
       " 'ile': <gensim.models.keyedvectors.Vocab at 0x1e860a25e08>,\n",
       " 'away': <gensim.models.keyedvectors.Vocab at 0x1e860a25e48>,\n",
       " 'from': <gensim.models.keyedvectors.Vocab at 0x1e860a25e88>,\n",
       " 'so': <gensim.models.keyedvectors.Vocab at 0x1e860a25ec8>,\n",
       " 'perceiue': <gensim.models.keyedvectors.Vocab at 0x1e860a25f08>,\n",
       " 'flye': <gensim.models.keyedvectors.Vocab at 0x1e860a25f48>,\n",
       " 'who': <gensim.models.keyedvectors.Vocab at 0x1e860a25f88>,\n",
       " 'else': <gensim.models.keyedvectors.Vocab at 0x1e860a25fc8>,\n",
       " 'men,': <gensim.models.keyedvectors.Vocab at 0x1e860a27048>,\n",
       " 'keepe': <gensim.models.keyedvectors.Vocab at 0x1e860a27088>,\n",
       " 'vs': <gensim.models.keyedvectors.Vocab at 0x1e860a270c8>,\n",
       " 'antony': <gensim.models.keyedvectors.Vocab at 0x1e860a27108>,\n",
       " 'brutus,': <gensim.models.keyedvectors.Vocab at 0x1e860a27148>,\n",
       " 'cassius,': <gensim.models.keyedvectors.Vocab at 0x1e860a27188>,\n",
       " 'caska,': <gensim.models.keyedvectors.Vocab at 0x1e860a271c8>,\n",
       " 'after': <gensim.models.keyedvectors.Vocab at 0x1e860a27208>,\n",
       " 'caes.': <gensim.models.keyedvectors.Vocab at 0x1e860a27248>,\n",
       " 'cask.': <gensim.models.keyedvectors.Vocab at 0x1e860a27288>,\n",
       " 'peace': <gensim.models.keyedvectors.Vocab at 0x1e860a272c8>,\n",
       " 'ho,': <gensim.models.keyedvectors.Vocab at 0x1e860a27308>,\n",
       " 'speakes': <gensim.models.keyedvectors.Vocab at 0x1e860a27348>,\n",
       " 'calp.': <gensim.models.keyedvectors.Vocab at 0x1e860a27388>,\n",
       " 'heere': <gensim.models.keyedvectors.Vocab at 0x1e860a273c8>,\n",
       " 'lord': <gensim.models.keyedvectors.Vocab at 0x1e860a27408>,\n",
       " 'stand': <gensim.models.keyedvectors.Vocab at 0x1e860a27448>,\n",
       " 'doth': <gensim.models.keyedvectors.Vocab at 0x1e860a27488>,\n",
       " 'run': <gensim.models.keyedvectors.Vocab at 0x1e860a274c8>,\n",
       " 'ant.': <gensim.models.keyedvectors.Vocab at 0x1e860a27508>,\n",
       " 'our': <gensim.models.keyedvectors.Vocab at 0x1e860a27548>,\n",
       " 'shake': <gensim.models.keyedvectors.Vocab at 0x1e860a27588>,\n",
       " 'off': <gensim.models.keyedvectors.Vocab at 0x1e860a275c8>,\n",
       " 'shall': <gensim.models.keyedvectors.Vocab at 0x1e860a27608>,\n",
       " 'sayes,': <gensim.models.keyedvectors.Vocab at 0x1e860a27648>,\n",
       " 'set': <gensim.models.keyedvectors.Vocab at 0x1e860a27688>,\n",
       " 'on,': <gensim.models.keyedvectors.Vocab at 0x1e860a276c8>,\n",
       " 'leaue': <gensim.models.keyedvectors.Vocab at 0x1e860a27708>,\n",
       " 'sooth.': <gensim.models.keyedvectors.Vocab at 0x1e860a27748>,\n",
       " 'bid': <gensim.models.keyedvectors.Vocab at 0x1e860a27788>,\n",
       " 'euery': <gensim.models.keyedvectors.Vocab at 0x1e860a277c8>,\n",
       " 'me?': <gensim.models.keyedvectors.Vocab at 0x1e860a27808>,\n",
       " 'tongue': <gensim.models.keyedvectors.Vocab at 0x1e860a27848>,\n",
       " 'caesar:': <gensim.models.keyedvectors.Vocab at 0x1e860a27888>,\n",
       " 'ides': <gensim.models.keyedvectors.Vocab at 0x1e860a278c8>,\n",
       " 'march': <gensim.models.keyedvectors.Vocab at 0x1e860a27908>,\n",
       " 'man': <gensim.models.keyedvectors.Vocab at 0x1e860a27948>,\n",
       " 'before': <gensim.models.keyedvectors.Vocab at 0x1e860a27988>,\n",
       " 'me,': <gensim.models.keyedvectors.Vocab at 0x1e860a279c8>,\n",
       " 'face': <gensim.models.keyedvectors.Vocab at 0x1e860a27a08>,\n",
       " 'cassi.': <gensim.models.keyedvectors.Vocab at 0x1e860a27a48>,\n",
       " 'fellow,': <gensim.models.keyedvectors.Vocab at 0x1e860a27a88>,\n",
       " 'come': <gensim.models.keyedvectors.Vocab at 0x1e860a27ac8>,\n",
       " 'againe,': <gensim.models.keyedvectors.Vocab at 0x1e860a27b08>,\n",
       " 'him:': <gensim.models.keyedvectors.Vocab at 0x1e860a27b48>,\n",
       " 'brut.': <gensim.models.keyedvectors.Vocab at 0x1e860a27b88>,\n",
       " '&': <gensim.models.keyedvectors.Vocab at 0x1e860a27bc8>,\n",
       " 'cass.': <gensim.models.keyedvectors.Vocab at 0x1e860a27c08>,\n",
       " 'some': <gensim.models.keyedvectors.Vocab at 0x1e860a27c48>,\n",
       " 'part': <gensim.models.keyedvectors.Vocab at 0x1e860a27c88>,\n",
       " 'spirit': <gensim.models.keyedvectors.Vocab at 0x1e860a27cc8>,\n",
       " 'antony:': <gensim.models.keyedvectors.Vocab at 0x1e860a27d08>,\n",
       " 'cassius': <gensim.models.keyedvectors.Vocab at 0x1e860a27d48>,\n",
       " 'eyes,': <gensim.models.keyedvectors.Vocab at 0x1e860a27d88>,\n",
       " 'shew': <gensim.models.keyedvectors.Vocab at 0x1e860a27dc8>,\n",
       " 'loue,': <gensim.models.keyedvectors.Vocab at 0x1e860a27e08>,\n",
       " 'was': <gensim.models.keyedvectors.Vocab at 0x1e860a27e48>,\n",
       " 'beare': <gensim.models.keyedvectors.Vocab at 0x1e860a27e88>,\n",
       " 'too': <gensim.models.keyedvectors.Vocab at 0x1e860a27ec8>,\n",
       " 'strange': <gensim.models.keyedvectors.Vocab at 0x1e860a27f08>,\n",
       " 'hand': <gensim.models.keyedvectors.Vocab at 0x1e860a27f48>,\n",
       " 'friend,': <gensim.models.keyedvectors.Vocab at 0x1e860a27f88>,\n",
       " 'loues': <gensim.models.keyedvectors.Vocab at 0x1e860a27fc8>,\n",
       " 'bru.': <gensim.models.keyedvectors.Vocab at 0x1e860a2b048>,\n",
       " 'looke,': <gensim.models.keyedvectors.Vocab at 0x1e860a2b088>,\n",
       " 'turne': <gensim.models.keyedvectors.Vocab at 0x1e860a2b0c8>,\n",
       " 'onely': <gensim.models.keyedvectors.Vocab at 0x1e860a2b108>,\n",
       " 'selfe,': <gensim.models.keyedvectors.Vocab at 0x1e860a2b148>,\n",
       " 'giue': <gensim.models.keyedvectors.Vocab at 0x1e860a2b188>,\n",
       " 'therefore': <gensim.models.keyedvectors.Vocab at 0x1e860a2b1c8>,\n",
       " 'friends': <gensim.models.keyedvectors.Vocab at 0x1e860a2b208>,\n",
       " 'any': <gensim.models.keyedvectors.Vocab at 0x1e860a2b248>,\n",
       " 'further': <gensim.models.keyedvectors.Vocab at 0x1e860a2b288>,\n",
       " 'brutus': <gensim.models.keyedvectors.Vocab at 0x1e860a2b2c8>,\n",
       " 'himselfe': <gensim.models.keyedvectors.Vocab at 0x1e860a2b308>,\n",
       " 'at': <gensim.models.keyedvectors.Vocab at 0x1e860a2b348>,\n",
       " 'loue': <gensim.models.keyedvectors.Vocab at 0x1e860a2b388>,\n",
       " 'other': <gensim.models.keyedvectors.Vocab at 0x1e860a2b3c8>,\n",
       " 'much': <gensim.models.keyedvectors.Vocab at 0x1e860a2b408>,\n",
       " 'meanes': <gensim.models.keyedvectors.Vocab at 0x1e860a2b448>,\n",
       " 'mine': <gensim.models.keyedvectors.Vocab at 0x1e860a2b488>,\n",
       " 'hath': <gensim.models.keyedvectors.Vocab at 0x1e860a2b4c8>,\n",
       " 'worthy': <gensim.models.keyedvectors.Vocab at 0x1e860a2b508>,\n",
       " 'tell': <gensim.models.keyedvectors.Vocab at 0x1e860a2b548>,\n",
       " 'brutus.': <gensim.models.keyedvectors.Vocab at 0x1e860a2b588>,\n",
       " 'cassius:': <gensim.models.keyedvectors.Vocab at 0x1e860a2b5c8>,\n",
       " 'things': <gensim.models.keyedvectors.Vocab at 0x1e860a2b608>,\n",
       " 'cassius.': <gensim.models.keyedvectors.Vocab at 0x1e860a2b648>,\n",
       " \"'tis\": <gensim.models.keyedvectors.Vocab at 0x1e860a2b688>,\n",
       " 'very': <gensim.models.keyedvectors.Vocab at 0x1e860a2b6c8>,\n",
       " 'such': <gensim.models.keyedvectors.Vocab at 0x1e860a2b708>,\n",
       " 'might': <gensim.models.keyedvectors.Vocab at 0x1e860a2b748>,\n",
       " 'noble': <gensim.models.keyedvectors.Vocab at 0x1e860a2b788>,\n",
       " 'had': <gensim.models.keyedvectors.Vocab at 0x1e860a2b7c8>,\n",
       " 'eyes': <gensim.models.keyedvectors.Vocab at 0x1e860a2b808>,\n",
       " 'seeke': <gensim.models.keyedvectors.Vocab at 0x1e860a2b848>,\n",
       " 'cas.': <gensim.models.keyedvectors.Vocab at 0x1e860a2b888>,\n",
       " 'since': <gensim.models.keyedvectors.Vocab at 0x1e860a2b8c8>,\n",
       " 'know,': <gensim.models.keyedvectors.Vocab at 0x1e860a2b908>,\n",
       " 'cannot': <gensim.models.keyedvectors.Vocab at 0x1e860a2b948>,\n",
       " 'well': <gensim.models.keyedvectors.Vocab at 0x1e860a2b988>,\n",
       " 'gentle': <gensim.models.keyedvectors.Vocab at 0x1e860a2b9c8>,\n",
       " 'were': <gensim.models.keyedvectors.Vocab at 0x1e860a2ba08>,\n",
       " 'common': <gensim.models.keyedvectors.Vocab at 0x1e860a2ba48>,\n",
       " 'or': <gensim.models.keyedvectors.Vocab at 0x1e860a2ba88>,\n",
       " 'did': <gensim.models.keyedvectors.Vocab at 0x1e860a2bac8>,\n",
       " 'vse': <gensim.models.keyedvectors.Vocab at 0x1e860a2bb08>,\n",
       " 'hold': <gensim.models.keyedvectors.Vocab at 0x1e860a2bb48>,\n",
       " 'feare,': <gensim.models.keyedvectors.Vocab at 0x1e860a2bb88>,\n",
       " 'people': <gensim.models.keyedvectors.Vocab at 0x1e860a2bbc8>,\n",
       " 'i,': <gensim.models.keyedvectors.Vocab at 0x1e860a2bc08>,\n",
       " 'feare': <gensim.models.keyedvectors.Vocab at 0x1e860a2bc48>,\n",
       " 'thinke': <gensim.models.keyedvectors.Vocab at 0x1e860a2bc88>,\n",
       " 'it,': <gensim.models.keyedvectors.Vocab at 0x1e860a2bcc8>,\n",
       " 'generall': <gensim.models.keyedvectors.Vocab at 0x1e860a2bd08>,\n",
       " 'honor': <gensim.models.keyedvectors.Vocab at 0x1e860a2bd48>,\n",
       " 'one': <gensim.models.keyedvectors.Vocab at 0x1e860a2bd88>,\n",
       " 'death': <gensim.models.keyedvectors.Vocab at 0x1e860a2bdc8>,\n",
       " 'looke': <gensim.models.keyedvectors.Vocab at 0x1e860a2be08>,\n",
       " 'both': <gensim.models.keyedvectors.Vocab at 0x1e860a2be48>,\n",
       " 'name': <gensim.models.keyedvectors.Vocab at 0x1e860a2be88>,\n",
       " 'well,': <gensim.models.keyedvectors.Vocab at 0x1e860a2bec8>,\n",
       " 'be,': <gensim.models.keyedvectors.Vocab at 0x1e860a2bf08>,\n",
       " 'you,': <gensim.models.keyedvectors.Vocab at 0x1e860a2bf48>,\n",
       " 'once,': <gensim.models.keyedvectors.Vocab at 0x1e860a2bf88>,\n",
       " 'angry': <gensim.models.keyedvectors.Vocab at 0x1e860a2bfc8>,\n",
       " 'word,': <gensim.models.keyedvectors.Vocab at 0x1e860a2e048>,\n",
       " 'hearts': <gensim.models.keyedvectors.Vocab at 0x1e860a2e088>,\n",
       " 'ere': <gensim.models.keyedvectors.Vocab at 0x1e860a2e0c8>,\n",
       " 'could': <gensim.models.keyedvectors.Vocab at 0x1e860a2e108>,\n",
       " 'so,': <gensim.models.keyedvectors.Vocab at 0x1e860a2e148>,\n",
       " 'man,': <gensim.models.keyedvectors.Vocab at 0x1e860a2e188>,\n",
       " 'him.': <gensim.models.keyedvectors.Vocab at 0x1e860a2e1c8>,\n",
       " 'fit': <gensim.models.keyedvectors.Vocab at 0x1e860a2e208>,\n",
       " 'him,': <gensim.models.keyedvectors.Vocab at 0x1e860a2e248>,\n",
       " 'marke': <gensim.models.keyedvectors.Vocab at 0x1e860a2e288>,\n",
       " 'how': <gensim.models.keyedvectors.Vocab at 0x1e860a2e2c8>,\n",
       " 'true,': <gensim.models.keyedvectors.Vocab at 0x1e860a2e308>,\n",
       " 'same': <gensim.models.keyedvectors.Vocab at 0x1e860a2e348>,\n",
       " 'whose': <gensim.models.keyedvectors.Vocab at 0x1e860a2e388>,\n",
       " 'world,': <gensim.models.keyedvectors.Vocab at 0x1e860a2e3c8>,\n",
       " 'romans': <gensim.models.keyedvectors.Vocab at 0x1e860a2e408>,\n",
       " 'titinius,': <gensim.models.keyedvectors.Vocab at 0x1e860a2e448>,\n",
       " 'sicke': <gensim.models.keyedvectors.Vocab at 0x1e860a2e488>,\n",
       " 'ye': <gensim.models.keyedvectors.Vocab at 0x1e860a2e4c8>,\n",
       " 'gods,': <gensim.models.keyedvectors.Vocab at 0x1e860a2e508>,\n",
       " 'should': <gensim.models.keyedvectors.Vocab at 0x1e860a2e548>,\n",
       " 'world': <gensim.models.keyedvectors.Vocab at 0x1e860a2e588>,\n",
       " 'like': <gensim.models.keyedvectors.Vocab at 0x1e860a2e5c8>,\n",
       " 'vnder': <gensim.models.keyedvectors.Vocab at 0x1e860a2e608>,\n",
       " 'selues': <gensim.models.keyedvectors.Vocab at 0x1e860a2e648>,\n",
       " 'yours': <gensim.models.keyedvectors.Vocab at 0x1e860a2e688>,\n",
       " 'them,': <gensim.models.keyedvectors.Vocab at 0x1e860a2e6c8>,\n",
       " 'caesar.': <gensim.models.keyedvectors.Vocab at 0x1e860a2e708>,\n",
       " 'hast': <gensim.models.keyedvectors.Vocab at 0x1e860a2e748>,\n",
       " 'went': <gensim.models.keyedvectors.Vocab at 0x1e860a2e788>,\n",
       " 'say': <gensim.models.keyedvectors.Vocab at 0x1e860a2e7c8>,\n",
       " 'rome': <gensim.models.keyedvectors.Vocab at 0x1e860a2e808>,\n",
       " 'indeed,': <gensim.models.keyedvectors.Vocab at 0x1e860a2e848>,\n",
       " 'man.': <gensim.models.keyedvectors.Vocab at 0x1e860a2e888>,\n",
       " 'heard': <gensim.models.keyedvectors.Vocab at 0x1e860a2e8c8>,\n",
       " 'nothing': <gensim.models.keyedvectors.Vocab at 0x1e860a2e908>,\n",
       " 'this,': <gensim.models.keyedvectors.Vocab at 0x1e860a2e948>,\n",
       " 'times': <gensim.models.keyedvectors.Vocab at 0x1e860a2e988>,\n",
       " 'meete': <gensim.models.keyedvectors.Vocab at 0x1e860a2e9c8>,\n",
       " 'high': <gensim.models.keyedvectors.Vocab at 0x1e860a2ea08>,\n",
       " 'then,': <gensim.models.keyedvectors.Vocab at 0x1e860a2ea48>,\n",
       " 'this:': <gensim.models.keyedvectors.Vocab at 0x1e860a2ea88>,\n",
       " 'rather': <gensim.models.keyedvectors.Vocab at 0x1e860a2eac8>,\n",
       " 'sonne': <gensim.models.keyedvectors.Vocab at 0x1e860a2eb08>,\n",
       " 'weake': <gensim.models.keyedvectors.Vocab at 0x1e860a2eb48>,\n",
       " 'words': <gensim.models.keyedvectors.Vocab at 0x1e860a2eb88>,\n",
       " 'thus': <gensim.models.keyedvectors.Vocab at 0x1e860a2ebc8>,\n",
       " 'fire': <gensim.models.keyedvectors.Vocab at 0x1e860a2ec08>,\n",
       " 'done,': <gensim.models.keyedvectors.Vocab at 0x1e860a2ec48>,\n",
       " 'caska': <gensim.models.keyedvectors.Vocab at 0x1e860a2ec88>,\n",
       " 'note': <gensim.models.keyedvectors.Vocab at 0x1e860a2ecc8>,\n",
       " 'day': <gensim.models.keyedvectors.Vocab at 0x1e860a2ed08>,\n",
       " 'so:': <gensim.models.keyedvectors.Vocab at 0x1e860a2ed48>,\n",
       " 'cicero': <gensim.models.keyedvectors.Vocab at 0x1e860a2ed88>,\n",
       " 'lookes': <gensim.models.keyedvectors.Vocab at 0x1e860a2edc8>,\n",
       " 'seene': <gensim.models.keyedvectors.Vocab at 0x1e860a2ee08>,\n",
       " 'capitoll': <gensim.models.keyedvectors.Vocab at 0x1e860a2ee48>,\n",
       " 'being': <gensim.models.keyedvectors.Vocab at 0x1e860a2ee88>,\n",
       " 'caes': <gensim.models.keyedvectors.Vocab at 0x1e860a2eec8>,\n",
       " 'sleepe': <gensim.models.keyedvectors.Vocab at 0x1e860a2ef08>,\n",
       " 'dangerous': <gensim.models.keyedvectors.Vocab at 0x1e860a2ef48>,\n",
       " 'not:': <gensim.models.keyedvectors.Vocab at 0x1e860a2ef88>,\n",
       " 'through': <gensim.models.keyedvectors.Vocab at 0x1e860a2efc8>,\n",
       " 'himselfe,': <gensim.models.keyedvectors.Vocab at 0x1e860a31048>,\n",
       " 'neuer': <gensim.models.keyedvectors.Vocab at 0x1e860a31088>,\n",
       " 'thee': <gensim.models.keyedvectors.Vocab at 0x1e860a310c8>,\n",
       " 'right': <gensim.models.keyedvectors.Vocab at 0x1e860a31108>,\n",
       " 'hand,': <gensim.models.keyedvectors.Vocab at 0x1e860a31148>,\n",
       " 'speake': <gensim.models.keyedvectors.Vocab at 0x1e860a31188>,\n",
       " 'not?': <gensim.models.keyedvectors.Vocab at 0x1e860a311c8>,\n",
       " 'crowne': <gensim.models.keyedvectors.Vocab at 0x1e860a31208>,\n",
       " \"offer'd\": <gensim.models.keyedvectors.Vocab at 0x1e860a31248>,\n",
       " 'him;': <gensim.models.keyedvectors.Vocab at 0x1e860a31288>,\n",
       " 'backe': <gensim.models.keyedvectors.Vocab at 0x1e860a312c8>,\n",
       " 'fell': <gensim.models.keyedvectors.Vocab at 0x1e860a31308>,\n",
       " 'last': <gensim.models.keyedvectors.Vocab at 0x1e860a31348>,\n",
       " 'cry': <gensim.models.keyedvectors.Vocab at 0x1e860a31388>,\n",
       " 'hee': <gensim.models.keyedvectors.Vocab at 0x1e860a313c8>,\n",
       " 'it:': <gensim.models.keyedvectors.Vocab at 0x1e860a31408>,\n",
       " 'it.': <gensim.models.keyedvectors.Vocab at 0x1e860a31448>,\n",
       " 'that,': <gensim.models.keyedvectors.Vocab at 0x1e860a31488>,\n",
       " 'still': <gensim.models.keyedvectors.Vocab at 0x1e860a314c8>,\n",
       " 'hands,': <gensim.models.keyedvectors.Vocab at 0x1e860a31508>,\n",
       " 'owne': <gensim.models.keyedvectors.Vocab at 0x1e860a31548>,\n",
       " 'part,': <gensim.models.keyedvectors.Vocab at 0x1e860a31588>,\n",
       " 'durst': <gensim.models.keyedvectors.Vocab at 0x1e860a315c8>,\n",
       " 'you:': <gensim.models.keyedvectors.Vocab at 0x1e860a31608>,\n",
       " 'no,': <gensim.models.keyedvectors.Vocab at 0x1e860a31648>,\n",
       " 'meane': <gensim.models.keyedvectors.Vocab at 0x1e860a31688>,\n",
       " 'sure': <gensim.models.keyedvectors.Vocab at 0x1e860a316c8>,\n",
       " 'doe': <gensim.models.keyedvectors.Vocab at 0x1e860a31708>,\n",
       " 'true': <gensim.models.keyedvectors.Vocab at 0x1e860a31748>,\n",
       " 'came': <gensim.models.keyedvectors.Vocab at 0x1e860a31788>,\n",
       " 'vnto': <gensim.models.keyedvectors.Vocab at 0x1e860a317c8>,\n",
       " 'downe,': <gensim.models.keyedvectors.Vocab at 0x1e860a31808>,\n",
       " 'beene': <gensim.models.keyedvectors.Vocab at 0x1e860a31848>,\n",
       " 'thing': <gensim.models.keyedvectors.Vocab at 0x1e860a31888>,\n",
       " 'three': <gensim.models.keyedvectors.Vocab at 0x1e860a318c8>,\n",
       " \"there's\": <gensim.models.keyedvectors.Vocab at 0x1e860a31908>,\n",
       " 'done': <gensim.models.keyedvectors.Vocab at 0x1e860a31948>,\n",
       " 'those': <gensim.models.keyedvectors.Vocab at 0x1e860a31988>,\n",
       " 'me.': <gensim.models.keyedvectors.Vocab at 0x1e860a319c8>,\n",
       " 'fare': <gensim.models.keyedvectors.Vocab at 0x1e860a31a08>,\n",
       " 'well.': <gensim.models.keyedvectors.Vocab at 0x1e860a31a48>,\n",
       " 'yet,': <gensim.models.keyedvectors.Vocab at 0x1e860a31a88>,\n",
       " 'remember': <gensim.models.keyedvectors.Vocab at 0x1e860a31ac8>,\n",
       " 'night,': <gensim.models.keyedvectors.Vocab at 0x1e860a31b08>,\n",
       " 'forth': <gensim.models.keyedvectors.Vocab at 0x1e860a31b48>,\n",
       " 'farewell': <gensim.models.keyedvectors.Vocab at 0x1e860a31b88>,\n",
       " 'enter.': <gensim.models.keyedvectors.Vocab at 0x1e860a31bc8>,\n",
       " 'now,': <gensim.models.keyedvectors.Vocab at 0x1e860a31c08>,\n",
       " 'better': <gensim.models.keyedvectors.Vocab at 0x1e860a31c48>,\n",
       " 'please': <gensim.models.keyedvectors.Vocab at 0x1e860a31c88>,\n",
       " 'will,': <gensim.models.keyedvectors.Vocab at 0x1e860a31cc8>,\n",
       " 'exit': <gensim.models.keyedvectors.Vocab at 0x1e860a31d08>,\n",
       " 'see,': <gensim.models.keyedvectors.Vocab at 0x1e860a31d48>,\n",
       " 'seuerall': <gensim.models.keyedvectors.Vocab at 0x1e860a31d88>,\n",
       " 'dayes': <gensim.models.keyedvectors.Vocab at 0x1e860a31dc8>,\n",
       " 'brought': <gensim.models.keyedvectors.Vocab at 0x1e860a31e08>,\n",
       " 'send': <gensim.models.keyedvectors.Vocab at 0x1e860a31e48>,\n",
       " 'left': <gensim.models.keyedvectors.Vocab at 0x1e860a31e88>,\n",
       " 'fire,': <gensim.models.keyedvectors.Vocab at 0x1e860a31ec8>,\n",
       " 'sword,': <gensim.models.keyedvectors.Vocab at 0x1e860a31f08>,\n",
       " 'against': <gensim.models.keyedvectors.Vocab at 0x1e860a31f48>,\n",
       " 'night': <gensim.models.keyedvectors.Vocab at 0x1e860a31f88>,\n",
       " 'euen': <gensim.models.keyedvectors.Vocab at 0x1e860a31fc8>,\n",
       " 'word': <gensim.models.keyedvectors.Vocab at 0x1e860a34048>,\n",
       " 'morrow': <gensim.models.keyedvectors.Vocab at 0x1e860a34088>,\n",
       " 'voyce': <gensim.models.keyedvectors.Vocab at 0x1e860a340c8>,\n",
       " 'this?': <gensim.models.keyedvectors.Vocab at 0x1e860a34108>,\n",
       " 'full': <gensim.models.keyedvectors.Vocab at 0x1e860a34148>,\n",
       " 'life,': <gensim.models.keyedvectors.Vocab at 0x1e860a34188>,\n",
       " 'cause,': <gensim.models.keyedvectors.Vocab at 0x1e860a341c8>,\n",
       " 'change': <gensim.models.keyedvectors.Vocab at 0x1e860a34208>,\n",
       " 'spirits,': <gensim.models.keyedvectors.Vocab at 0x1e860a34248>,\n",
       " 'not,': <gensim.models.keyedvectors.Vocab at 0x1e860a34288>,\n",
       " 'dead,': <gensim.models.keyedvectors.Vocab at 0x1e860a342c8>,\n",
       " 'here': <gensim.models.keyedvectors.Vocab at 0x1e860a34308>,\n",
       " 'strong': <gensim.models.keyedvectors.Vocab at 0x1e860a34348>,\n",
       " 'life': <gensim.models.keyedvectors.Vocab at 0x1e860a34388>,\n",
       " 'beares': <gensim.models.keyedvectors.Vocab at 0x1e860a343c8>,\n",
       " 'base': <gensim.models.keyedvectors.Vocab at 0x1e860a34408>,\n",
       " 'vile': <gensim.models.keyedvectors.Vocab at 0x1e860a34448>,\n",
       " 'stay': <gensim.models.keyedvectors.Vocab at 0x1e860a34488>,\n",
       " 'cinna.': <gensim.models.keyedvectors.Vocab at 0x1e860a344c8>,\n",
       " 'cinna,': <gensim.models.keyedvectors.Vocab at 0x1e860a34508>,\n",
       " 'metellus': <gensim.models.keyedvectors.Vocab at 0x1e860a34548>,\n",
       " 'two': <gensim.models.keyedvectors.Vocab at 0x1e860a34588>,\n",
       " 'take': <gensim.models.keyedvectors.Vocab at 0x1e860a345c8>,\n",
       " 'vs.': <gensim.models.keyedvectors.Vocab at 0x1e860a34608>,\n",
       " 'decius': <gensim.models.keyedvectors.Vocab at 0x1e860a34648>,\n",
       " 'trebonius': <gensim.models.keyedvectors.Vocab at 0x1e860a34688>,\n",
       " 'appeare': <gensim.models.keyedvectors.Vocab at 0x1e860a346c8>,\n",
       " 'vs,': <gensim.models.keyedvectors.Vocab at 0x1e860a34708>,\n",
       " 'awake': <gensim.models.keyedvectors.Vocab at 0x1e860a34748>,\n",
       " 'lucius,': <gensim.models.keyedvectors.Vocab at 0x1e860a34788>,\n",
       " 'lucius.': <gensim.models.keyedvectors.Vocab at 0x1e860a347c8>,\n",
       " 'luc.': <gensim.models.keyedvectors.Vocab at 0x1e860a34808>,\n",
       " 'lord?': <gensim.models.keyedvectors.Vocab at 0x1e860a34848>,\n",
       " 'call': <gensim.models.keyedvectors.Vocab at 0x1e860a34888>,\n",
       " 'is,': <gensim.models.keyedvectors.Vocab at 0x1e860a348c8>,\n",
       " 'least': <gensim.models.keyedvectors.Vocab at 0x1e860a34908>,\n",
       " 'kill': <gensim.models.keyedvectors.Vocab at 0x1e860a34948>,\n",
       " 'found': <gensim.models.keyedvectors.Vocab at 0x1e860a34988>,\n",
       " 'lye': <gensim.models.keyedvectors.Vocab at 0x1e860a349c8>,\n",
       " 'day:': <gensim.models.keyedvectors.Vocab at 0x1e860a34a08>,\n",
       " 'first': <gensim.models.keyedvectors.Vocab at 0x1e860a34a48>,\n",
       " 'bring': <gensim.models.keyedvectors.Vocab at 0x1e860a34a88>,\n",
       " 'reade': <gensim.models.keyedvectors.Vocab at 0x1e860a34ac8>,\n",
       " 'tooke': <gensim.models.keyedvectors.Vocab at 0x1e860a34b08>,\n",
       " 'body': <gensim.models.keyedvectors.Vocab at 0x1e860a34b48>,\n",
       " 'little': <gensim.models.keyedvectors.Vocab at 0x1e860a34b88>,\n",
       " 'brother': <gensim.models.keyedvectors.Vocab at 0x1e860a34bc8>,\n",
       " 'wilt': <gensim.models.keyedvectors.Vocab at 0x1e860a34c08>,\n",
       " 'none': <gensim.models.keyedvectors.Vocab at 0x1e860a34c48>,\n",
       " 'hide': <gensim.models.keyedvectors.Vocab at 0x1e860a34c88>,\n",
       " 'night:': <gensim.models.keyedvectors.Vocab at 0x1e860a34cc8>,\n",
       " 'roman': <gensim.models.keyedvectors.Vocab at 0x1e860a34d08>,\n",
       " 'you.': <gensim.models.keyedvectors.Vocab at 0x1e860a34d48>,\n",
       " 'welcome': <gensim.models.keyedvectors.Vocab at 0x1e860a34d88>,\n",
       " 'cymber': <gensim.models.keyedvectors.Vocab at 0x1e860a34dc8>,\n",
       " 'decius.': <gensim.models.keyedvectors.Vocab at 0x1e860a34e08>,\n",
       " 'breake': <gensim.models.keyedvectors.Vocab at 0x1e860a34e48>,\n",
       " 'heere?': <gensim.models.keyedvectors.Vocab at 0x1e860a34e88>,\n",
       " 'cin.': <gensim.models.keyedvectors.Vocab at 0x1e860a34ec8>,\n",
       " 'heere,': <gensim.models.keyedvectors.Vocab at 0x1e860a34f08>,\n",
       " 'hands': <gensim.models.keyedvectors.Vocab at 0x1e860a34f48>,\n",
       " 'cause': <gensim.models.keyedvectors.Vocab at 0x1e860a34f88>,\n",
       " 'romans,': <gensim.models.keyedvectors.Vocab at 0x1e860a34fc8>,\n",
       " 'blood': <gensim.models.keyedvectors.Vocab at 0x1e860a38048>,\n",
       " 'him?': <gensim.models.keyedvectors.Vocab at 0x1e860a38088>,\n",
       " 'antony,': <gensim.models.keyedvectors.Vocab at 0x1e860a380c8>,\n",
       " 'farre': <gensim.models.keyedvectors.Vocab at 0x1e860a38108>,\n",
       " 'seeme': <gensim.models.keyedvectors.Vocab at 0x1e860a38148>,\n",
       " 'caius': <gensim.models.keyedvectors.Vocab at 0x1e860a38188>,\n",
       " 'cut': <gensim.models.keyedvectors.Vocab at 0x1e860a381c8>,\n",
       " 'death,': <gensim.models.keyedvectors.Vocab at 0x1e860a38208>,\n",
       " \"let's\": <gensim.models.keyedvectors.Vocab at 0x1e860a38248>,\n",
       " 'friends,': <gensim.models.keyedvectors.Vocab at 0x1e860a38288>,\n",
       " 'stirre': <gensim.models.keyedvectors.Vocab at 0x1e860a382c8>,\n",
       " 'dye': <gensim.models.keyedvectors.Vocab at 0x1e860a38308>,\n",
       " 'liue,': <gensim.models.keyedvectors.Vocab at 0x1e860a38348>,\n",
       " 'peace,': <gensim.models.keyedvectors.Vocab at 0x1e860a38388>,\n",
       " 'whether': <gensim.models.keyedvectors.Vocab at 0x1e860a383c8>,\n",
       " 'fetch': <gensim.models.keyedvectors.Vocab at 0x1e860a38408>,\n",
       " \"wee'l\": <gensim.models.keyedvectors.Vocab at 0x1e860a38448>,\n",
       " 'por.': <gensim.models.keyedvectors.Vocab at 0x1e860a38488>,\n",
       " 'gaue': <gensim.models.keyedvectors.Vocab at 0x1e860a384c8>,\n",
       " 'lord,': <gensim.models.keyedvectors.Vocab at 0x1e860a38508>,\n",
       " 'portia': <gensim.models.keyedvectors.Vocab at 0x1e860a38548>,\n",
       " 'within': <gensim.models.keyedvectors.Vocab at 0x1e860a38588>,\n",
       " 'place': <gensim.models.keyedvectors.Vocab at 0x1e860a385c8>,\n",
       " 'talke': <gensim.models.keyedvectors.Vocab at 0x1e860a38608>,\n",
       " 'honourable': <gensim.models.keyedvectors.Vocab at 0x1e860a38648>,\n",
       " 'deere': <gensim.models.keyedvectors.Vocab at 0x1e860a38688>,\n",
       " 'heart': <gensim.models.keyedvectors.Vocab at 0x1e860a386c8>,\n",
       " 'thee,': <gensim.models.keyedvectors.Vocab at 0x1e860a38708>,\n",
       " 'lucius': <gensim.models.keyedvectors.Vocab at 0x1e860a38748>,\n",
       " 'ligarius,': <gensim.models.keyedvectors.Vocab at 0x1e860a38788>,\n",
       " 'boy,': <gensim.models.keyedvectors.Vocab at 0x1e860a387c8>,\n",
       " 'cai.': <gensim.models.keyedvectors.Vocab at 0x1e860a38808>,\n",
       " 'braue': <gensim.models.keyedvectors.Vocab at 0x1e860a38848>,\n",
       " 'seruant.': <gensim.models.keyedvectors.Vocab at 0x1e860a38888>,\n",
       " 'ser.': <gensim.models.keyedvectors.Vocab at 0x1e860a388c8>,\n",
       " 'house': <gensim.models.keyedvectors.Vocab at 0x1e860a38908>,\n",
       " 'end': <gensim.models.keyedvectors.Vocab at 0x1e860a38948>,\n",
       " 'mighty': <gensim.models.keyedvectors.Vocab at 0x1e860a38988>,\n",
       " 'come,': <gensim.models.keyedvectors.Vocab at 0x1e860a389c8>,\n",
       " 'house,': <gensim.models.keyedvectors.Vocab at 0x1e860a38a08>,\n",
       " 'mark': <gensim.models.keyedvectors.Vocab at 0x1e860a38a48>,\n",
       " 'deci.': <gensim.models.keyedvectors.Vocab at 0x1e860a38a88>,\n",
       " 'she': <gensim.models.keyedvectors.Vocab at 0x1e860a38ac8>,\n",
       " 'pardon': <gensim.models.keyedvectors.Vocab at 0x1e860a38b08>,\n",
       " 'publius': <gensim.models.keyedvectors.Vocab at 0x1e860a38b48>,\n",
       " 'prepare': <gensim.models.keyedvectors.Vocab at 0x1e860a38b88>,\n",
       " 'blame': <gensim.models.keyedvectors.Vocab at 0x1e860a38bc8>,\n",
       " 'will:': <gensim.models.keyedvectors.Vocab at 0x1e860a38c08>,\n",
       " 'thee.': <gensim.models.keyedvectors.Vocab at 0x1e860a38c48>,\n",
       " 'traitors': <gensim.models.keyedvectors.Vocab at 0x1e860a38c88>,\n",
       " 'suite': <gensim.models.keyedvectors.Vocab at 0x1e860a38cc8>,\n",
       " 'read': <gensim.models.keyedvectors.Vocab at 0x1e860a38d08>,\n",
       " 'friend': <gensim.models.keyedvectors.Vocab at 0x1e860a38d48>,\n",
       " 'master': <gensim.models.keyedvectors.Vocab at 0x1e860a38d88>,\n",
       " \"lou'd\": <gensim.models.keyedvectors.Vocab at 0x1e860a38dc8>,\n",
       " 'dead': <gensim.models.keyedvectors.Vocab at 0x1e860a38e08>,\n",
       " 'though': <gensim.models.keyedvectors.Vocab at 0x1e860a38e48>,\n",
       " 'bloody': <gensim.models.keyedvectors.Vocab at 0x1e860a38e88>,\n",
       " 'wrong': <gensim.models.keyedvectors.Vocab at 0x1e860a38ec8>,\n",
       " 'marcus': <gensim.models.keyedvectors.Vocab at 0x1e860a38f08>,\n",
       " \"did'st\": <gensim.models.keyedvectors.Vocab at 0x1e860a38f48>,\n",
       " 'so.': <gensim.models.keyedvectors.Vocab at 0x1e860a38f88>,\n",
       " 'octauius': <gensim.models.keyedvectors.Vocab at 0x1e860a38fc8>,\n",
       " 'yong': <gensim.models.keyedvectors.Vocab at 0x1e860a3c048>,\n",
       " 'octauius,': <gensim.models.keyedvectors.Vocab at 0x1e860a3c088>,\n",
       " '2.': <gensim.models.keyedvectors.Vocab at 0x1e860a3c0c8>,\n",
       " '3.': <gensim.models.keyedvectors.Vocab at 0x1e860a3c108>,\n",
       " '1.': <gensim.models.keyedvectors.Vocab at 0x1e860a3c148>,\n",
       " '4.': <gensim.models.keyedvectors.Vocab at 0x1e860a3c188>,\n",
       " '1': <gensim.models.keyedvectors.Vocab at 0x1e860a3c1c8>,\n",
       " '3': <gensim.models.keyedvectors.Vocab at 0x1e860a3c208>,\n",
       " '4': <gensim.models.keyedvectors.Vocab at 0x1e860a3c248>,\n",
       " '2': <gensim.models.keyedvectors.Vocab at 0x1e860a3c288>,\n",
       " 'heart,': <gensim.models.keyedvectors.Vocab at 0x1e860a3c2c8>,\n",
       " 'away,': <gensim.models.keyedvectors.Vocab at 0x1e860a3c308>,\n",
       " 'cinna': <gensim.models.keyedvectors.Vocab at 0x1e860a3c348>,\n",
       " 'octa.': <gensim.models.keyedvectors.Vocab at 0x1e860a3c388>,\n",
       " 'meet': <gensim.models.keyedvectors.Vocab at 0x1e860a3c3c8>,\n",
       " 'lucillius,': <gensim.models.keyedvectors.Vocab at 0x1e860a3c408>,\n",
       " 'titinius': <gensim.models.keyedvectors.Vocab at 0x1e860a3c448>,\n",
       " 'pindarus': <gensim.models.keyedvectors.Vocab at 0x1e860a3c488>,\n",
       " 'lucil.': <gensim.models.keyedvectors.Vocab at 0x1e860a3c4c8>,\n",
       " 'ill': <gensim.models.keyedvectors.Vocab at 0x1e860a3c508>,\n",
       " 'lucillius': <gensim.models.keyedvectors.Vocab at 0x1e860a3c548>,\n",
       " 'tent': <gensim.models.keyedvectors.Vocab at 0x1e860a3c588>,\n",
       " 'messala': <gensim.models.keyedvectors.Vocab at 0x1e860a3c5c8>,\n",
       " 'messala,': <gensim.models.keyedvectors.Vocab at 0x1e860a3c608>,\n",
       " 'philippi': <gensim.models.keyedvectors.Vocab at 0x1e860a3c648>,\n",
       " 'messa.': <gensim.models.keyedvectors.Vocab at 0x1e860a3c688>,\n",
       " 'enemy': <gensim.models.keyedvectors.Vocab at 0x1e860a3c6c8>,\n",
       " 'tit.': <gensim.models.keyedvectors.Vocab at 0x1e860a3c708>,\n",
       " 'sword': <gensim.models.keyedvectors.Vocab at 0x1e860a3c748>,\n",
       " 'cato,': <gensim.models.keyedvectors.Vocab at 0x1e860a3c788>,\n",
       " 'alarum.': <gensim.models.keyedvectors.Vocab at 0x1e860a3c7c8>,\n",
       " 'titin.': <gensim.models.keyedvectors.Vocab at 0x1e860a3c808>,\n",
       " 'strato,': <gensim.models.keyedvectors.Vocab at 0x1e860a3c848>,\n",
       " 'clit.': <gensim.models.keyedvectors.Vocab at 0x1e860a3c888>}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import gensim.models as gmodels\n",
    "\n",
    "\n",
    "# get sentece from the raw txt\n",
    "f = open('data_home_assignment_2\\shakespeare-caesar.txt')\n",
    "raw = f.read()\n",
    "sent_tokens = nltk.sent_tokenize(raw)\n",
    "\n",
    "#preprocessing the sentence: strip unnecesary data normalizing cases\n",
    "sentences=[]\n",
    "for sent in sent_tokens:\n",
    "    s = re.split(r'\\s+',sent.strip()) #using re to remove \\n\\n \\t and so on\n",
    "    sentence = [w.lower() for w in s] #normalizing cases\n",
    "    # print(string.punctuation)\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "model = gmodels.Word2Vec(sentences=sentences)\n",
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word similarity with pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of understanding how good the obtained embeddings are is the word similarity task. The file 'sim353.csv' contains a set of word pairs as well as their similarity as judged by humans (e.g., tiger,tiger,10). \n",
    "Also we provide a set of pre-trained embeddings in 'embeddings.pickle' in the form of a dictionary with words as keys and embeddings as values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Consider each word pair in the given file ('sim353.csv') and calculate the cosine similarity between them and then rank the word pairs based on the similarity value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Human (mean)</th>\n",
       "      <th>cos_sim</th>\n",
       "      <th>rank (cos.sim.)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>money</td>\n",
       "      <td>cash</td>\n",
       "      <td>9.08</td>\n",
       "      <td>0.815317</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>money</td>\n",
       "      <td>cash</td>\n",
       "      <td>9.15</td>\n",
       "      <td>0.815317</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>football</td>\n",
       "      <td>soccer</td>\n",
       "      <td>9.03</td>\n",
       "      <td>0.814274</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>championship</td>\n",
       "      <td>tournament</td>\n",
       "      <td>8.36</td>\n",
       "      <td>0.807413</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>man</td>\n",
       "      <td>woman</td>\n",
       "      <td>8.30</td>\n",
       "      <td>0.804799</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>football</td>\n",
       "      <td>basketball</td>\n",
       "      <td>6.81</td>\n",
       "      <td>0.799937</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>king</td>\n",
       "      <td>queen</td>\n",
       "      <td>8.58</td>\n",
       "      <td>0.759618</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>car</td>\n",
       "      <td>automobile</td>\n",
       "      <td>8.94</td>\n",
       "      <td>0.733075</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>physics</td>\n",
       "      <td>chemistry</td>\n",
       "      <td>7.35</td>\n",
       "      <td>0.729284</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>weather</td>\n",
       "      <td>forecast</td>\n",
       "      <td>8.34</td>\n",
       "      <td>0.716078</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>street</td>\n",
       "      <td>avenue</td>\n",
       "      <td>8.88</td>\n",
       "      <td>0.708214</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>credit</td>\n",
       "      <td>card</td>\n",
       "      <td>8.06</td>\n",
       "      <td>0.689643</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>aluminum</td>\n",
       "      <td>metal</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.684565</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>type</td>\n",
       "      <td>kind</td>\n",
       "      <td>8.97</td>\n",
       "      <td>0.679077</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>coast</td>\n",
       "      <td>shore</td>\n",
       "      <td>9.10</td>\n",
       "      <td>0.677802</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>sex</td>\n",
       "      <td>6.77</td>\n",
       "      <td>0.677000</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>television</td>\n",
       "      <td>radio</td>\n",
       "      <td>6.77</td>\n",
       "      <td>0.674091</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>seafood</td>\n",
       "      <td>lobster</td>\n",
       "      <td>8.70</td>\n",
       "      <td>0.674077</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>computer</td>\n",
       "      <td>software</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.672374</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>day</td>\n",
       "      <td>summer</td>\n",
       "      <td>3.94</td>\n",
       "      <td>0.670856</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>fuck</td>\n",
       "      <td>sex</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.670561</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>calculation</td>\n",
       "      <td>computation</td>\n",
       "      <td>8.44</td>\n",
       "      <td>0.669189</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>vodka</td>\n",
       "      <td>gin</td>\n",
       "      <td>8.46</td>\n",
       "      <td>0.668546</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bread</td>\n",
       "      <td>butter</td>\n",
       "      <td>6.19</td>\n",
       "      <td>0.667959</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>midday</td>\n",
       "      <td>noon</td>\n",
       "      <td>9.29</td>\n",
       "      <td>0.665748</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>drink</td>\n",
       "      <td>eat</td>\n",
       "      <td>6.87</td>\n",
       "      <td>0.664106</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>law</td>\n",
       "      <td>lawyer</td>\n",
       "      <td>8.38</td>\n",
       "      <td>0.655002</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>psychology</td>\n",
       "      <td>psychiatry</td>\n",
       "      <td>8.08</td>\n",
       "      <td>0.650045</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>psychology</td>\n",
       "      <td>science</td>\n",
       "      <td>6.71</td>\n",
       "      <td>0.650004</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>shower</td>\n",
       "      <td>thunderstorm</td>\n",
       "      <td>6.31</td>\n",
       "      <td>0.237645</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>tiger</td>\n",
       "      <td>fauna</td>\n",
       "      <td>5.62</td>\n",
       "      <td>0.226339</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>reason</td>\n",
       "      <td>hypertension</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.210041</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>lad</td>\n",
       "      <td>wizard</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.207744</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>sign</td>\n",
       "      <td>recess</td>\n",
       "      <td>2.38</td>\n",
       "      <td>0.206303</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>cup</td>\n",
       "      <td>tableware</td>\n",
       "      <td>6.85</td>\n",
       "      <td>0.206225</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>asylum</td>\n",
       "      <td>madhouse</td>\n",
       "      <td>8.87</td>\n",
       "      <td>0.199657</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>cup</td>\n",
       "      <td>object</td>\n",
       "      <td>3.69</td>\n",
       "      <td>0.197506</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>size</td>\n",
       "      <td>prominence</td>\n",
       "      <td>5.31</td>\n",
       "      <td>0.191225</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>cup</td>\n",
       "      <td>substance</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.190121</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>territory</td>\n",
       "      <td>kilometer</td>\n",
       "      <td>5.28</td>\n",
       "      <td>0.183836</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>production</td>\n",
       "      <td>hike</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.183246</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>king</td>\n",
       "      <td>cabbage</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.182654</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>chord</td>\n",
       "      <td>smile</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.181856</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>decoration</td>\n",
       "      <td>valor</td>\n",
       "      <td>5.63</td>\n",
       "      <td>0.181139</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>crane</td>\n",
       "      <td>implement</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.179740</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>impartiality</td>\n",
       "      <td>interest</td>\n",
       "      <td>5.16</td>\n",
       "      <td>0.172755</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>delay</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.171557</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>monk</td>\n",
       "      <td>slave</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.160540</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>precedent</td>\n",
       "      <td>collection</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.159243</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>glass</td>\n",
       "      <td>magician</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.158012</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>monk</td>\n",
       "      <td>oracle</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.138562</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>precedent</td>\n",
       "      <td>cognition</td>\n",
       "      <td>2.81</td>\n",
       "      <td>0.124228</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>precedent</td>\n",
       "      <td>group</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.122940</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>noon</td>\n",
       "      <td>string</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.122765</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>cup</td>\n",
       "      <td>entity</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.095603</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>professor</td>\n",
       "      <td>cucumber</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.088859</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>cup</td>\n",
       "      <td>artifact</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.045669</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>rooster</td>\n",
       "      <td>voyage</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.038883</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>tiger</td>\n",
       "      <td>organism</td>\n",
       "      <td>4.77</td>\n",
       "      <td>0.036870</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>335 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word 1        Word 2  Human (mean)   cos_sim  rank (cos.sim.)\n",
       "2           tiger         tiger         10.00  1.000000                1\n",
       "90          money          cash          9.08  0.815317                2\n",
       "30          money          cash          9.15  0.815317                3\n",
       "38       football        soccer          9.03  0.814274                4\n",
       "279  championship    tournament          8.36  0.807413                5\n",
       "289           man         woman          8.30  0.804799                6\n",
       "39       football    basketball          6.81  0.799937                7\n",
       "33           king         queen          8.58  0.759618                8\n",
       "59            car    automobile          8.94  0.733075                9\n",
       "48        physics     chemistry          7.35  0.729284               10\n",
       "331       weather      forecast          8.34  0.716078               11\n",
       "229        street        avenue          8.88  0.708214               12\n",
       "213        credit          card          8.06  0.689643               13\n",
       "319      aluminum         metal          7.83  0.684565               14\n",
       "220          type          kind          8.97  0.679077               15\n",
       "63          coast         shore          9.10  0.677802               16\n",
       "0            love           sex          6.77  0.677000               17\n",
       "9      television         radio          6.77  0.674091               18\n",
       "270       seafood       lobster          8.70  0.674077               19\n",
       "252      computer      software          8.50  0.672374               20\n",
       "282           day        summer          3.94  0.670856               21\n",
       "37           fuck           sex          9.44  0.670561               22\n",
       "241   calculation   computation          8.44  0.669189               23\n",
       "51          vodka           gin          8.46  0.668546               24\n",
       "12          bread        butter          6.19  0.667959               25\n",
       "66         midday          noon          9.29  0.665748               26\n",
       "56          drink           eat          6.87  0.664106               27\n",
       "42            law        lawyer          8.38  0.655002               28\n",
       "108    psychology    psychiatry          8.08  0.650045               29\n",
       "116    psychology       science          6.71  0.650004               30\n",
       "..            ...           ...           ...       ...              ...\n",
       "329        shower  thunderstorm          6.31  0.237645              306\n",
       "106         tiger         fauna          5.62  0.226339              307\n",
       "192        reason  hypertension          2.31  0.210041              308\n",
       "84            lad        wizard          0.92  0.207744              309\n",
       "156          sign        recess          2.38  0.206303              310\n",
       "134           cup     tableware          6.85  0.206225              311\n",
       "64         asylum      madhouse          8.87  0.199657              312\n",
       "137           cup        object          3.69  0.197506              313\n",
       "312          size    prominence          5.31  0.191225              314\n",
       "141           cup     substance          1.92  0.190121              315\n",
       "206     territory     kilometer          5.28  0.183836              316\n",
       "235    production          hike          1.75  0.183246              317\n",
       "32           king       cabbage          0.23  0.182654              318\n",
       "85          chord         smile          0.54  0.181856              319\n",
       "168    decoration         valor          5.63  0.181139              320\n",
       "73          crane     implement          2.69  0.179740              321\n",
       "226  impartiality      interest          5.16  0.172755              322\n",
       "171         delay        racism          1.19  0.171557              323\n",
       "82           monk         slave          0.92  0.160540              324\n",
       "130     precedent    collection          2.50  0.159243              325\n",
       "86          glass      magician          2.08  0.158012              326\n",
       "76           monk        oracle          5.00  0.138562              327\n",
       "128     precedent     cognition          2.81  0.124228              328\n",
       "131     precedent         group          1.77  0.122940              329\n",
       "87           noon        string          0.54  0.122765              330\n",
       "138           cup        entity          2.15  0.095603              331\n",
       "31      professor      cucumber          0.31  0.088859              332\n",
       "136           cup      artifact          2.92  0.045669              333\n",
       "88        rooster        voyage          0.62  0.038883              334\n",
       "105         tiger      organism          4.77  0.036870              335\n",
       "\n",
       "[335 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "#read from pick and csv\n",
    "pickle_in = open('data_home_assignment_2\\embeddings.pickle','rb')\n",
    "embeddings = pickle.load(pickle_in)\n",
    "csv_in = pd.read_csv('data_home_assignment_2\\sim353.csv',delimiter=',')\n",
    "df_sim353 = pd.DataFrame(csv_in)\n",
    "\n",
    "word1 = df_sim353['Word 1']\n",
    "word2 = df_sim353['Word 2']\n",
    "sim_human = df_sim353['Human (mean)']\n",
    "pairlen = len(word1)\n",
    "\n",
    "# calculate the cosine similarity between word pair, \n",
    "# if a word not in the embedding, \n",
    "# the similarity will be replaced by the similarity measured by human \n",
    "words = embeddings.keys()\n",
    "cos_sim = []\n",
    "for i in range(pairlen):\n",
    "    if word1[i] in words and word2[i] in words:\n",
    "        a = embeddings[word1[i]]\n",
    "        b = embeddings[word2[i]]\n",
    "        # calculate cosin similarity\n",
    "        cos_sim.append(np.inner(a,b)/(norm(a)*norm(b)))\n",
    "    else:\n",
    "        cos_sim.append(sim_human[i]/10) #divid 10 to change it into value in [0,1]\n",
    "\n",
    "# add a cloumn cosine similarity sorted the table according to it\n",
    "df = df_sim353.copy(deep=True)\n",
    "df['cos_sim']= cos_sim\n",
    "df_cos_sorted = df.sort_values(by=['cos_sim'],ascending=False)\n",
    "rank = range(1,pairlen+1)\n",
    "df_cos_sorted['rank (cos.sim.)'] = rank\n",
    "display(df_cos_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Rank the word pairs based on the similarity values as determined by humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Human (mean)</th>\n",
       "      <th>cos_sim</th>\n",
       "      <th>rank (human)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>fuck</td>\n",
       "      <td>sex</td>\n",
       "      <td>9.44</td>\n",
       "      <td>0.670561</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>midday</td>\n",
       "      <td>noon</td>\n",
       "      <td>9.29</td>\n",
       "      <td>0.665748</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>journey</td>\n",
       "      <td>voyage</td>\n",
       "      <td>9.29</td>\n",
       "      <td>0.643200</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>dollar</td>\n",
       "      <td>buck</td>\n",
       "      <td>9.22</td>\n",
       "      <td>0.426276</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>money</td>\n",
       "      <td>cash</td>\n",
       "      <td>9.15</td>\n",
       "      <td>0.815317</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>coast</td>\n",
       "      <td>shore</td>\n",
       "      <td>9.10</td>\n",
       "      <td>0.677802</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>money</td>\n",
       "      <td>cash</td>\n",
       "      <td>9.08</td>\n",
       "      <td>0.815317</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>money</td>\n",
       "      <td>currency</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0.522259</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>football</td>\n",
       "      <td>soccer</td>\n",
       "      <td>9.03</td>\n",
       "      <td>0.814274</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>magician</td>\n",
       "      <td>wizard</td>\n",
       "      <td>9.02</td>\n",
       "      <td>0.513534</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>type</td>\n",
       "      <td>kind</td>\n",
       "      <td>8.97</td>\n",
       "      <td>0.679077</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>gem</td>\n",
       "      <td>jewel</td>\n",
       "      <td>8.96</td>\n",
       "      <td>0.578007</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>car</td>\n",
       "      <td>automobile</td>\n",
       "      <td>8.94</td>\n",
       "      <td>0.733075</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>street</td>\n",
       "      <td>avenue</td>\n",
       "      <td>8.88</td>\n",
       "      <td>0.708214</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>asylum</td>\n",
       "      <td>madhouse</td>\n",
       "      <td>8.87</td>\n",
       "      <td>0.199657</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>boy</td>\n",
       "      <td>lad</td>\n",
       "      <td>8.83</td>\n",
       "      <td>0.531423</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>environment</td>\n",
       "      <td>ecology</td>\n",
       "      <td>8.81</td>\n",
       "      <td>0.535516</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>furnace</td>\n",
       "      <td>stove</td>\n",
       "      <td>8.79</td>\n",
       "      <td>0.585582</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>seafood</td>\n",
       "      <td>lobster</td>\n",
       "      <td>8.70</td>\n",
       "      <td>0.674077</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>mile</td>\n",
       "      <td>kilometer</td>\n",
       "      <td>8.66</td>\n",
       "      <td>0.646130</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>king</td>\n",
       "      <td>queen</td>\n",
       "      <td>8.58</td>\n",
       "      <td>0.759618</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>murder</td>\n",
       "      <td>manslaughter</td>\n",
       "      <td>8.53</td>\n",
       "      <td>0.569898</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>money</td>\n",
       "      <td>bank</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.602752</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>computer</td>\n",
       "      <td>software</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.672374</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>vodka</td>\n",
       "      <td>gin</td>\n",
       "      <td>8.46</td>\n",
       "      <td>0.668546</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>planet</td>\n",
       "      <td>star</td>\n",
       "      <td>8.45</td>\n",
       "      <td>0.519998</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>calculation</td>\n",
       "      <td>computation</td>\n",
       "      <td>8.44</td>\n",
       "      <td>0.669189</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>money</td>\n",
       "      <td>dollar</td>\n",
       "      <td>8.42</td>\n",
       "      <td>0.614731</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>law</td>\n",
       "      <td>lawyer</td>\n",
       "      <td>8.38</td>\n",
       "      <td>0.655002</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>cup</td>\n",
       "      <td>article</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.258458</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>sign</td>\n",
       "      <td>recess</td>\n",
       "      <td>2.38</td>\n",
       "      <td>0.206303</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>problem</td>\n",
       "      <td>airport</td>\n",
       "      <td>2.38</td>\n",
       "      <td>0.282557</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>reason</td>\n",
       "      <td>hypertension</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.210041</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>direction</td>\n",
       "      <td>combination</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.400209</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>cup</td>\n",
       "      <td>entity</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.095603</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>glass</td>\n",
       "      <td>magician</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.158012</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>cemetery</td>\n",
       "      <td>woodland</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.379716</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>possibility</td>\n",
       "      <td>girl</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.310412</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>cup</td>\n",
       "      <td>substance</td>\n",
       "      <td>1.92</td>\n",
       "      <td>0.190121</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>forest</td>\n",
       "      <td>graveyard</td>\n",
       "      <td>1.85</td>\n",
       "      <td>0.298963</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>energy</td>\n",
       "      <td>secretary</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.340388</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>month</td>\n",
       "      <td>hotel</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.333641</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>stock</td>\n",
       "      <td>egg</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.339092</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>precedent</td>\n",
       "      <td>group</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.122940</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>production</td>\n",
       "      <td>hike</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.183246</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>stock</td>\n",
       "      <td>phone</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.306102</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>holy</td>\n",
       "      <td>sex</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.266039</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>drink</td>\n",
       "      <td>ear</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.270334</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>delay</td>\n",
       "      <td>racism</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.171557</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>lad</td>\n",
       "      <td>wizard</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.207744</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>stock</td>\n",
       "      <td>life</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.411336</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>monk</td>\n",
       "      <td>slave</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.160540</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>stock</td>\n",
       "      <td>jaguar</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.276747</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>sugar</td>\n",
       "      <td>approach</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.249256</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>rooster</td>\n",
       "      <td>voyage</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.038883</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>noon</td>\n",
       "      <td>string</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.122765</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>chord</td>\n",
       "      <td>smile</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.181856</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>professor</td>\n",
       "      <td>cucumber</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.088859</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>king</td>\n",
       "      <td>cabbage</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.182654</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>335 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word 1        Word 2  Human (mean)   cos_sim  rank (human)\n",
       "2          tiger         tiger         10.00  1.000000             1\n",
       "37          fuck           sex          9.44  0.670561             2\n",
       "66        midday          noon          9.29  0.665748             3\n",
       "61       journey        voyage          9.29  0.643200             4\n",
       "249       dollar          buck          9.22  0.426276             5\n",
       "30         money          cash          9.15  0.815317             6\n",
       "63         coast         shore          9.10  0.677802             7\n",
       "90         money          cash          9.08  0.815317             8\n",
       "91         money      currency          9.04  0.522259             9\n",
       "38      football        soccer          9.03  0.814274            10\n",
       "65      magician        wizard          9.02  0.513534            11\n",
       "220         type          kind          8.97  0.679077            12\n",
       "60           gem         jewel          8.96  0.578007            13\n",
       "59           car    automobile          8.94  0.733075            14\n",
       "229       street        avenue          8.88  0.708214            15\n",
       "64        asylum      madhouse          8.87  0.199657            16\n",
       "62           boy           lad          8.83  0.531423            17\n",
       "287  environment       ecology          8.81  0.535516            18\n",
       "67       furnace         stove          8.79  0.585582            19\n",
       "270      seafood       lobster          8.70  0.674077            20\n",
       "157         mile     kilometer          8.66  0.646130            21\n",
       "33          king         queen          8.58  0.759618            22\n",
       "291       murder  manslaughter          8.53  0.569898            23\n",
       "95         money          bank          8.50  0.602752            24\n",
       "252     computer      software          8.50  0.672374            25\n",
       "51         vodka           gin          8.46  0.668546            26\n",
       "119       planet          star          8.45  0.519998            27\n",
       "241  calculation   computation          8.44  0.669189            28\n",
       "89         money        dollar          8.42  0.614731            29\n",
       "42           law        lawyer          8.38  0.655002            30\n",
       "..           ...           ...           ...       ...           ...\n",
       "135          cup       article          2.40  0.258458           306\n",
       "156         sign        recess          2.38  0.206303           307\n",
       "211      problem       airport          2.38  0.282557           308\n",
       "192       reason  hypertension          2.31  0.210041           309\n",
       "227    direction   combination          2.25  0.400209           310\n",
       "138          cup        entity          2.15  0.095603           311\n",
       "86         glass      magician          2.08  0.158012           312\n",
       "77      cemetery      woodland          2.08  0.379716           313\n",
       "300  possibility          girl          1.94  0.310412           314\n",
       "141          cup     substance          1.92  0.190121           315\n",
       "80        forest     graveyard          1.85  0.298963           316\n",
       "145       energy     secretary          1.81  0.340388           317\n",
       "219        month         hotel          1.81  0.333641           318\n",
       "23         stock           egg          1.81  0.339092           319\n",
       "131    precedent         group          1.77  0.122940           320\n",
       "235   production          hike          1.75  0.183246           321\n",
       "21         stock         phone          1.62  0.306102           322\n",
       "36          holy           sex          1.62  0.266039           323\n",
       "54         drink           ear          1.31  0.270334           324\n",
       "171        delay        racism          1.19  0.171557           325\n",
       "84           lad        wizard          0.92  0.207744           326\n",
       "26         stock          life          0.92  0.411336           327\n",
       "82          monk         slave          0.92  0.160540           328\n",
       "22         stock        jaguar          0.92  0.276747           329\n",
       "308        sugar      approach          0.88  0.249256           330\n",
       "88       rooster        voyage          0.62  0.038883           331\n",
       "87          noon        string          0.54  0.122765           332\n",
       "85         chord         smile          0.54  0.181856           333\n",
       "31     professor      cucumber          0.31  0.088859           334\n",
       "32          king       cabbage          0.23  0.182654           335\n",
       "\n",
       "[335 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_human_sorted = df.sort_values(by=['Human (mean)'],ascending=False)\n",
    "\n",
    "df_human_sorted['rank (human)'] = rank\n",
    "df_human_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Calculate the spearman rank correlation between the two ranked list of word-pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.6450611570036899, pvalue=8.391971318047086e-41)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# sorted this two lists according to index, each row represents the same word pair\n",
    "df_cos = df_cos_sorted.sort_index()\n",
    "df_human = df_human_sorted.sort_index()\n",
    "data1 = df_human['rank (human)']\n",
    "data2 = df_cos['rank (cos.sim.)']\n",
    "spearmanr(data1, data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification with CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a set of news articles which are to be labelled 1 or 0. The data is split into 3 groups (train/test/validation). Each group is further divided into 2 files which consists of the text(ending with \\_X.p) and the label (ending with \\_y.p) respectively. Each datapoint is a list of words and they are all accumulated in a list forming a list of lists. The label file is a list of labels (0/1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will design a character-level CNN. So the first task would be to obtain a one-hot encoding for the characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the vocabulary of caracters is provided for your reference.. \n",
    "vocabulary = list(\"\"\" abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'/\\|_@#$%&*+-=<>()[]{}\"\"\")\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '0': 27, '1': 28, '2': 29, '3': 30, '4': 31, '5': 32, '6': 33, '7': 34, '8': 35, '9': 36, '-': 59, ',': 38, ';': 39, '.': 40, '!': 41, '?': 42, ':': 43, \"'\": 44, '/': 45, '\\\\': 46, '|': 47, '_': 48, '@': 49, '#': 50, '$': 51, '%': 52, '': 53, '&': 54, '*': 55, '': 56, '': 57, '+': 58, '=': 60, '<': 61, '>': 62, '(': 63, ')': 64, '[': 65, ']': 66, '{': 67, '}': 68}\n"
     ]
    }
   ],
   "source": [
    "def characterEncoding(vocabulary):\n",
    "    c2v = {} # dictionary with key as a character and one-hot encoding as value\n",
    "    # write your code snippet here..\n",
    "    \n",
    "    # dictionary that maps every char to an unique int based on position\n",
    "    c2v = dict((c,i) for i,c in enumerate(vocabulary))\n",
    "    return c2v\n",
    "#test\n",
    "c2v= characterEncoding(vocabulary)\n",
    "print(c2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode a sentence as a 2D matrix with each row representing the one-hot encoding of a character in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.]),\n",
       " array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.]),\n",
       " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.]),\n",
       " array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sentence2tensor(sentence,c2v):\n",
    "    # Here we assume that all characters in sentences are within the alphabet\n",
    "    # TODO\n",
    "    # input: sentence\n",
    "    # output: a 2D matrix with each row is a vector filled with 0 and 1, and value is where the 1 is.\n",
    "   \n",
    "    matrix = []\n",
    "    for c in sentence:\n",
    "        if c in c2v:\n",
    "            vector = (np.zeros(len(c2v)+1))\n",
    "            vector[c2v[c]] = 1\n",
    "            matrix.append(vector)\n",
    "\n",
    "        #print(vector)\n",
    "    #vector = [[0 if char != letter else 1 for char in c2v] #             for letter in sentence]\n",
    "    return matrix\n",
    "\n",
    "#test\n",
    "# sentence2tensor(\"hello world\",c2v)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a dataset class for feeding data to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['news_computer_test_X.p', 'news_computer_test_y.p', 'news_computer_train_X.p', 'news_computer_train_y.p', 'news_computer_val_X.p', 'news_computer_val_y.p']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import fnmatch\n",
    "import torch\n",
    "\n",
    "ListOfFile = os.listdir('data_home_assignment_2/Data/')\n",
    "print(ListOfFile)\n",
    "# Read a file and split into lines\n",
    "context = []\n",
    "            \n",
    "def ProcessingData(file):\n",
    "    filename = ''\n",
    "    pre = 'data_home_assignment_2/Data/'\n",
    "    if file == \"test\":\n",
    "        filename = pre+'news_computer_test_X.p'\n",
    "        labels = pre+'news_computer_test_y.p'\n",
    "    elif file == \"train\":\n",
    "        filename = pre+'news_computer_train_X.p'\n",
    "        labels = pre+'news_computer_train_y.p'\n",
    "    elif file == \"validation\":\n",
    "        filename = pre+'news_computer_val_X.p'\n",
    "        labels = pre+'news_computer_val_y.p'\n",
    "            \n",
    "    lines = pickle.load(open(filename,\"rb\"))\n",
    "    lst = []\n",
    "    shortest = 99999\n",
    "    for line in lines:\n",
    "        sentence = ''\n",
    "        for word in line:\n",
    "            sentence+=word\n",
    "        if len(sentence) < shortest:\n",
    "            shortest = len(sentence)\n",
    "        l= sentence2tensor(sentence[0:100],c2v)\n",
    "        lst.append(l)\n",
    "    \n",
    "    label = torch.LongTensor(pickle.load(open(labels,\"rb\")))\n",
    "    tensor = (torch.LongTensor(lst)).unsqueeze(0)\n",
    "    return tensor.long(),label.long()\n",
    "# a,n = ProcessingData('train')\n",
    "# print(a)\n",
    "#lines = pickle.load(open('data_home_assignment_2/Data/news_computer_test_X.p',\"rb\"))\n",
    "#n.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class NewsDataset(Dataset): \n",
    "     def __init__(self, names, labels):\n",
    "        super(NewsDataset,self).__init__()\n",
    "        self.names = names\n",
    "        self.labels = labels\n",
    "    \n",
    "     def __len__(self):\n",
    "        return len(self.names)\n",
    "    \n",
    "     def __getitem__(self,index):\n",
    "        #self.names = Variable(np.random.rand(1,1,100,69))\n",
    "        #(self.names).unsqueeze(0)\n",
    "        self.names = Variable(self.names).type(torch.LongTensor)\n",
    "        self.labels = Variable(self.labels).type(torch.LongTensor)\n",
    "    \n",
    "        return self.names[index],self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 100, 69])\n",
      "<__main__.NewsDataset object at 0x000001163D55F348>\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "train_dataset,train_labels = ProcessingData(\"test\")\n",
    "#print(train_dataset.unsqueeze(0).size())\n",
    "#print(len(train_dataset[0]))\n",
    "print(train_dataset.size())\n",
    "\n",
    "train_data = NewsDataset(train_dataset,train_labels)\n",
    "print(train_data)\n",
    "train = DataLoader(train_dataset, batch_size=32)\n",
    "print(len(train))\n",
    "#test = DataLoader('data_home_assignment_2/Data/news_computer_train_X.p', batch_size=batch_size)\n",
    "#validation = DataLoader('data_home_assignment_2/Data/news_computer_val_X.p', batch_size=batch_size)\n",
    "#validation,test\n",
    "#DataLoader(batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a model class with 2 layers of convolutions each followed by a ReLU unit. The model should have linear layer which maps to the classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "       \n",
    "        self.cnn_layers = Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            Conv2d(1, 4, kernel_size=3, stride=1, padding=1),\n",
    "            ReLU(inplace=True),\n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            ReLU(inplace=True),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(5828, 2)\n",
    "      \n",
    "\n",
    "#         define layers\n",
    "#         self.conv1 = nn.Conv2d(1, 4, kernel_size=3, stride=1)\n",
    "#         #self.pool =  nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.conv2 = nn.Conv2d(4, 4, kernel_size=3, stride=1)\n",
    "\n",
    "        #self.fc1 = nn.Linear(5828, 2)\n",
    "        #self.out = nn.Linear(64,2)\n",
    "\n",
    "  # define forward function\n",
    "    def forward(self, x):\n",
    "        # conv 1 with relu afterwards\n",
    "#         x = self.conv1(x)\n",
    "#         x = F.relu(x)\n",
    "#         # x = self.pool(x)\n",
    "#         # conv 2 with relu afterwards\n",
    "#         x = F.relu(self.conv2(x))\n",
    "        \n",
    "#         # fc1\n",
    "        #x = x.reshape(-1, 12*4*4)\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "    \n",
    "#         x = x.view(-1, 5828) \n",
    "#         x = self.fc1(x)\n",
    "\n",
    "        # output\n",
    "        #x = self.out(x)\n",
    "        #return x\n",
    "#         self.cnn_layers = Sequential(\n",
    "#             # Defining a 2D convolution layer\n",
    "#             Conv2d(1, 4, kernel_size=3, stride=1, padding=1),\n",
    "#             ReLU(inplace=True),\n",
    "#             # Defining another 2D convolution layer\n",
    "#             Conv2d(4,4, kernel_size=3, stride=1, padding=1),\n",
    "#             ReLU(inplace=True),\n",
    "\n",
    "\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x=F.relu(self.conv1(x)) \n",
    "#         #Size changes from (18, 32, 32) to (18, 16, 16)\n",
    "#         x=F.relu(self.conv2(x)) \n",
    "#         x=x.view(-1, 18*16*16)\n",
    "#         x=F.relu(self.fc1(x))\n",
    "        \n",
    "        #x = self.cnn_layers(x)\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        #x = self.linear_layers(x)\n",
    "       # return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a train function which trains on the train dataset. You can use binary cross entropy as your loss function and Adam as your optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "    \n",
    "def train(model, train_dataset, epochs=5, batch_size=32, learning_rate=0.0001):\n",
    "  \n",
    "    #We get data packed in batsch size by using dataloader\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset,32)\n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(),lr= learning_rate)\n",
    "    with tqdm(total=int(len(train_dataset)/batch_size)) as pbar:\n",
    "        for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(train_dataloader, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, labels = data\n",
    "                inputs= inputs.to(dtype=torch.int64)\n",
    "                labels = labels.to(dtype=torch.int64)\n",
    "                print(inputs.type())\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                if i % 200 == 199:    # print every 2000 mini-batches\n",
    "                    print('[%d, %5d] loss: %.3f' %\n",
    "                          (epoch + 1, i + 1, running_loss / 200))\n",
    "                    running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "        # write your code snippet here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a test function which takes the trained model and test dataset and outputs the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_dataset, batch_size=32):\n",
    "    \n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the testset: %d %%' % (100 * correct / total))\n",
    "    \n",
    "    # write your code snippet here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.NewsDataset object at 0x000001163D55F348>\n",
      "torch.LongTensor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'weight' in call to _thnn_conv2d_forward",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-046ef7b41cf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;31m#test(model, test_dataset)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-1d3e3ddbf505>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_dataset, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[1;31m# forward + backward + optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-a18a7c242672>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m#         # fc1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m#x = x.reshape(-1, 12*4*4)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnn_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tm_env\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tm_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tm_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tm_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 342\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'weight' in call to _thnn_conv2d_forward"
     ]
    }
   ],
   "source": [
    "# putting it altogether\n",
    "train_data,train_labels = ProcessingData(\"train\")\n",
    "test_data,test_labels = ProcessingData(\"test\")\n",
    "  \n",
    "train_dataset = NewsDataset(train_data,train_labels)    \n",
    "test_dataset = NewsDataset(test_data,test_labels)\n",
    "\n",
    "print(train_dataset)\n",
    "model = Classifier()\n",
    "\n",
    "train(model, train_dataset)\n",
    "#test(model, test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
